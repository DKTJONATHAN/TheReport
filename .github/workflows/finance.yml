name: Global News Cycle (US, UK, CA Focus)

on:
  schedule:
    # 6 Alternate Intervals for Tech and Finance
    - cron: '0 2 * * *'   # Finance
    - cron: '0 6 * * *'   # Tech
    - cron: '0 10 * * *'  # Finance
    - cron: '0 14 * * *'  # Tech
    - cron: '0 18 * * *'  # Finance
    - cron: '0 22 * * *'  # Tech
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (finance or tech)'
        required: false
        default: ''

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Generate Content
        env:
          GEMINI_POOL: "${{ secrets.GEMINI_API_KEY }},${{ secrets.GEMINI_API_KEY1 }},${{ secrets.GEMINI_API_KEY2 }},${{ secrets.GEMINI_WRITE_KEY }}"
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, hashlib, time
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright
          from email.utils import parsedate_to_datetime
          import textwrap

          # --- 1. CONFIGURATION ---
          def get_clean_pool(v): return [k.strip() for k in v.split(",") if k.strip()]
          gem_pool = get_clean_pool(os.environ.get("GEMINI_POOL", ""))

          def clean_dashes(text):
              if not text: return ""
              text = text.replace('\u2014', ' - ').replace('\u2013', '-')
              text = re.sub(r'\s+', ' ', text)
              return text.strip()

          def get_internal_context():
              posts_dir = os.environ.get("POSTS_DIR")
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 4))
              context = "PAST STORIES (LINK 1 OF THESE):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- {t}: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def run_gemini(p):
              random.shuffle(gem_pool)
              models_to_try = ["gemini-2.0-flash", "gemini-2.5-flash"]
              for model_name in models_to_try:
                  for k in gem_pool:
                      try:
                          client = genai.Client(api_key=k)
                          return client.models.generate_content(
                              model=model_name, 
                              contents=p,
                              config=types.GenerateContentConfig(temperature=0.7, max_output_tokens=8192)
                          ).text.strip()
                      except Exception as e:
                          time.sleep(1)
                          continue
              return None

          # --- 2. TOPIC ROTATION ---
          cron_map = {
              '0 2 * * *': 'finance',
              '0 6 * * *': 'tech',
              '0 10 * * *': 'finance',
              '0 14 * * *': 'tech',
              '0 18 * * *': 'finance',
              '0 22 * * *': 'tech'
          }
          topic = os.environ.get('MANUAL_TOPIC', '').lower() or cron_map.get(os.environ.get('CRON_SCHEDULE', ''), 'finance')
          
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          # --- 3. GOOGLE NEWS RSS DIRECT ---
          rss_topic_map = {'finance': 'BUSINESS', 'tech': 'TECHNOLOGY'}
          rss_category = rss_topic_map.get(topic, 'BUSINESS')
          
          rss_url = f"https://news.google.com/rss/headlines/section/topic/{rss_category}?hl=en-US&gl=US&ceid=US:en"

          print(f"üì° Fetching latest {topic} news from Google RSS...")
          try:
              resp = requests.get(rss_url, timeout=10)
              soup = BeautifulSoup(resp.content, "xml")
              items = soup.find_all("item")
          except Exception as e:
              print(f"‚ùå RSS Error: {e}")
              exit(0)

          valid_candidates = []
          for item in items:
              link = item.link.text.strip()
              pub_str = item.pubDate.text.strip()
              title = re.sub(r'\s+-\s+[^-]+$', '', item.title.text).strip()
              
              try:
                  pdate = parsedate_to_datetime(pub_str)
                  if datetime.datetime.now(pdate.tzinfo) - pdate > datetime.timedelta(hours=2):
                      continue
              except: continue
              
              valid_candidates.append({"url": link, "title": title})

          if not valid_candidates:
              print("‚ùå No valid news published within the last 2 hours.")
              exit(0)

          # --- 4. TRIPLE-LAYER URL DECODER ---
          def resolve_google_news_url(url):
              print(f"üîó Decoding Google News URL...")
              try:
                  headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0"}
                  
                  # Layer 1: Check HTML for redirect tags
                  res = requests.get(url, headers=headers, timeout=10)
                  match = re.search(r'data-n-v="(https?://[^"]+)"', res.text)
                  if match: return match.group(1)
                  match = re.search(r'<a[^>]*href="(https?://[^"]+)"', res.text)
                  if match and "google.com" not in match.group(1): return match.group(1)

                  # Layer 2: Batchexecute API Decode
                  from urllib.parse import urlparse
                  token = urlparse(url).path.split('/')[-1]
                  param = f'["garturlreq",[["en-US","US",["FINANCE_TOP_INDICES","WEB_TEST_1_0_0"],null,null,1,1,"US:en",null,null,null,null,null,null,null,0,5],"en-US","US",true,[2,4,8],1,true,"661099999",0,0,null,0],"{token}"]'
                  data = {'f.req': json.dumps([[["Fbv4je", param, "null", "generic"]]])}
                  rpc_res = requests.post("https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je", data=data, headers=headers, timeout=10)
                  urls = re.findall(r'https?://[^"\\]+', rpc_res.text)
                  for u in urls:
                      if "google.com" not in u and "gstatic.com" not in u:
                          return u
                          
                  return res.url
              except Exception as e:
                  print(f"‚ö†Ô∏è Decoder warning: {e}")
                  return url

          # --- 5. FREE SCRAPER ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox", "--ignore-certificate-errors"])
                  context = browser.new_context(
                      user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                      locale="en-US",
                      ignore_https_errors=True
                  )
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

                  try:
                      page.goto(url, timeout=45000, wait_until="domcontentloaded")
                      
                      # Safely catch Chromium crash
                      if "chrome-error" in page.url:
                          print("‚ö†Ô∏è Blocked by chrome-error (headless detection).")
                          return None

                      try: page.wait_for_load_state("networkidle", timeout=10000)
                      except: pass

                      content = page.content()
                      soup = BeautifulSoup(content, "html.parser")
                      
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 600:
                              article_text = chunk
                              break
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                      return article_text
                  except Exception as e: 
                      print(f"‚ö†Ô∏è Playwright Error: {e}")
                      return None
                  finally: browser.close()

          # --- 6. PROCESS ARTICLES ---
          full_raw_text, final_hash, target_title = None, None, None
          
          for a in valid_candidates:
              raw_url = a['url']
              target_title = a['title']
              
              t_hash = hashlib.md5(target_title.encode()).hexdigest()
              if t_hash in memory: continue
              
              # Resolve the URL FIRST to avoid Playwright crashes
              resolved_url = resolve_google_news_url(raw_url)
              
              # Attempt to Scrape with Playwright
              text = scrape_article_free(resolved_url)
              
              # Extreme Fallback: If Playwright fails, use basic requests
              if not text or len(text) < 600:
                  print("‚ö†Ô∏è Playwright blocked, falling back to Requests...")
                  try:
                      res = requests.get(resolved_url, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0"}, timeout=15)
                      soup = BeautifulSoup(res.text, "html.parser")
                      paras = soup.find_all("p")
                      fallback_text = "\n\n".join([p.text.strip() for p in paras if len(p.text.strip()) > 50])
                      if len(fallback_text) > 600:
                          text = fallback_text
                  except: pass

              if text and len(text) > 600:
                  full_raw_text = text
                  final_hash = t_hash
                  break 

          if not full_raw_text: print("‚ùå All recent URLs failed to scrape or were already posted."); exit(0)

          # --- 7. THE PROMPT ---
          internal_linking = get_internal_context()
          
          finance_instruction = ""
          if topic == 'finance':
              finance_instruction = "CRITICAL REQUIREMENT: You MUST include a distinct section at the end titled 'What This Means For You' answering exactly what this news means to the reader as an everyday consumer or investor in the US, UK, or Canada."

          prompt = f"""OPERATE AS A PROFESSIONAL JOURNALIST AND REPORTER.
          SOURCE MATERIAL: {full_raw_text[:12000]}
          TARGET AUDIENCE: Readers in the USA, Canada, and the UK. Ensure currency, context, and impacts are tailored to these regions.
          {internal_linking}
          
          MANDATE: 
          - Analyze the latest impact of this story with a professional reporting tone.
          - Tone: Objective, analytical, and authoritative.
          {finance_instruction}
          
          *** ANONYMITY RULE (STRICT) ***
          - Do NOT mention the original news source, publication name, wire service, or rival journalists anywhere in the text (e.g., do NOT say "according to Reuters", "Bloomberg reports", "The New York Times stated"). Present the facts as your own independent dispatch.
          
          *** HEADLINE RULES ***
          1. Keep the headline as close as possible to the original title: "{target_title}". Do not invent a completely new title, just polish it for clarity and remove any brand names of other news outlets.
          
          *** IMAGE RULE ***
          - Return a specific 2-to-3 word descriptive phrase representing the visual scene for the image search. Do NOT use a single broad keyword. 
          - Example: 'trading floor screens', 'server room cables', 'empty office building'.
          
          OUTPUT FORMAT:
          TITLE: [Headline]
          DESCRIPTION: [Summary]
          IMAGE_CAPTION: [Caption]
          CATEGORY: [{topic.title()}]
          TAGS: [3 tags]
          IMAGE_KEYWORD: [2-3 word specific phrase]
          BODY: [Full Article]"""

          output = run_gemini(prompt)
          if not output: exit(1)

          # --- 8. PARSE & CLEAN ---
          def extract(key, text):
              m = re.search(rf"^{key}:\s*(.*)", text, re.M | re.I)
              return m.group(1).strip() if m else None

          parsed = {
              "TITLE": extract("TITLE", output) or target_title,
              "DESC": extract("DESCRIPTION", output) or "Full story inside.",
              "CAPTION": extract("IMAGE_CAPTION", output) or "Developing story.",
              "IMG": extract("IMAGE_KEYWORD", output) or f"{topic} news concept",
              "BODY": output.split("BODY:")[-1].strip()
          }
          if re.search(r"BODY:\s*(.*)", output, re.S | re.I):
              parsed["BODY"] = re.search(r"BODY:\s*(.*)", output, re.S | re.I).group(1).strip()

          for k in parsed:
              parsed[k] = clean_dashes(parsed[k])
              parsed[k] = re.sub(r'(?i)(In conclusion|To summarize|Conclusion),?.*', '', parsed[k]).strip()
          parsed["BODY"] = clean_dashes(parsed["BODY"])

          # --- 9. SMART IMAGE SEARCH ---
          date_now = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          u_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          
          def get_img(q):
              defaults = {
                  'finance': "https://images.unsplash.com/photo-1611974765270-ca1258634369?w=1200", 
                  'tech': "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=1200" 
              }
              fallback = defaults.get(topic, defaults['finance'])
              if not u_key: return fallback
              
              search_terms = [q, f"{topic} concept", topic]
              
              for term in search_terms:
                  try:
                      clean_term = re.sub(r'[^a-zA-Z0-9 ]', '', term).strip()
                      print(f"üì∑ Unsplash Search: {clean_term}")
                      url = f"https://api.unsplash.com/photos/random?query={clean_term}&orientation=landscape&client_id={u_key}"
                      r = requests.get(url, timeout=5)
                      if r.status_code == 200:
                          return r.json()['urls']['regular']
                  except: continue
              return fallback
          
          img_url = get_img(parsed['IMG'])

          schema_json = {
              "@context": "https://schema.org",
              "@type": "NewsArticle",
              "headline": parsed['TITLE'],
              "description": parsed['DESC'],
              "datePublished": date_now,
              "author": {"@type": "Person", "name": "Global Desk", "url": "https://jonathanmwaniki.co.ke"},
              "image": [img_url]
          }

          final_md = textwrap.dedent(f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESC'].replace('"', "'")}"
          date: "{date_now}"
          author: "Global Desk"
          image: "{img_url}"
          imageCaption: "{parsed['CAPTION'].replace('"', "'")}"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          {parsed['BODY']}

          <script type="application/ld+json">
          {json.dumps(schema_json, indent=2)}
          </script>
          """).strip()

          out_dir = os.environ.get("POSTS_DIR")
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f: f.write(final_md)
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"‚úÖ Published: {slug}")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: latest global dispatch"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'