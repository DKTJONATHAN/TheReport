import os, json, datetime, requests, re, random, time
          from google import genai
          from google.genai import types
          from ntscraper import Nitter

          # --- 1. CONFIGURATION & TOPIC ---
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour

          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['sports', 'technology', 'business']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              else: topic = 'business'

          print(f"üéØ Topic: {topic.upper()}")

          # --- 2. FETCH REAL-TIME NEWS (Robust Scraper) ---
          SOURCES = {
              'sports': ['MozzartSportKe', 'CapitalSportKE', 'NationSports'],
              'technology': ['Techweez', 'Kachwanya', 'AndroidKenya'],
              'business': ['BusinessDailyJC', 'StandardKenya', 'NationAfrica']
          }

          # Hardcoded list of robust instances (Cloudflare-friendly)
          NITTER_INSTANCES = [
              'https://nitter.privacydev.net',
              'https://nitter.poast.org',
              'https://nitter.lucabased.xyz',
              'https://nitter.net'
          ]

          # Initialize WITHOUT checking instances (saves time and avoids startup crash)
          scraper = Nitter(log_level=1, skip_instance_check=True)

          target_accounts = SOURCES.get(topic, SOURCES['business'])
          tweet_data = None
          source_handle = ""

          for handle in target_accounts:
              print(f"üï∑Ô∏è Attempting to scrape @{handle}...")
              
              # Try multiple instances for each handle
              for instance in NITTER_INSTANCES:
                  try:
                      print(f"   Using instance: {instance}...")
                      # Explicitly pass the instance URL here
                      results = scraper.get_tweets(handle, mode='user', number=5, instance=instance)
                      tweets = results.get('tweets', [])
                      
                      if not tweets:
                          print("   No tweets found or empty response.")
                          continue

                      # Filter for valid non-retweet news
                      for t in tweets:
                          if not t['is-retweet'] and len(t['text']) > 60:
                              tweet_data = t
                              source_handle = handle
                              break
                      
                      if tweet_data: 
                          print("   ‚úÖ Success!")
                          break # Break instance loop
                          
                  except Exception as e:
                      print(f"   ‚ö†Ô∏è Instance {instance} failed: {e}")
                      continue
              
              if tweet_data: break # Break account loop if we found news

          if not tweet_data:
              print("‚ùå No news found from any source after trying all instances.")
              exit(0)

          # Normalize data
          raw_text = tweet_data['text']
          title = ' '.join(raw_text.split()[:10]) + "..."
          snippet = raw_text
          source = f"@{source_handle} (Twitter)"
          pub_date = tweet_data['date']

          print(f"üì∫ Found story: {snippet[:80]}... ({source})")

          # --- 3. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              # Updated fallback to avoid issues
              fallback = f"https://image.pollinations.ai/prompt/minimalist-journalistic-photo-of-{query.replace(' ', '-')}-news?width=1200&height=630&nologo=true"
              if not access_key:
                  return fallback
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=5) # Reduced timeout
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except: pass
              return fallback

          # --- 4. REPORTING-TONE PROMPT ---
          hooks = [
             "Straight reporting: What happened, when, where, who, why.",
             "Key facts first: Timeline of events.",
             "Source-led: What experts/officials said."
          ]
          selected_hook = random.choice(hooks)

          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.

          NEWS SOURCE (TWEET):
          Source: {source}
          Date: {pub_date}
          Content: "{snippet}"

          TASK: Write a SHORT news update (approx 600 words) based on this report.
          - Contextualize this tweet.
          - Opening: {selected_hook}

          OUTPUT FORMAT (exact):
          TITLE: [News-style headline, max 60 chars]
          DESCRIPTION: [Summary, max 160 chars]
          CATEGORY: [{topic.capitalize()}]
          TAGS: [3-5 tags, comma separated]
          IMAGE_KEYWORD: [2-3 words for image]
          BODY:
          [Article body with ## Headers]
          '''

          # --- 5. GEMINI 3 CALL ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          model_id = "gemini-2.0-flash" # Switched to 2.0 Flash (more stable/faster than 3-preview for this)
          
          print(f"ü§ñ Generating with {model_id}...")
          
          try:
              response = client.models.generate_content(
                  model=model_id,
                  contents=prompt,
                  config=types.GenerateContentConfig(temperature=0.1)
              )
              full_text = response.text.strip()
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              exit(1)

          # --- 6. PARSE & SAVE ---
          parsed = { "TITLE": title[:60], "DESCRIPTION": snippet[:160], "CATEGORY": topic.capitalize(), "TAGS": f"{topic},news", "IMAGE_KEYWORD": topic, "BODY": "" }
          current_section = None

          for line in full_text.splitlines():
              clean_line = line.strip().replace("**", "")
              if clean_line.startswith("TITLE:"): parsed["TITLE"] = clean_line.replace("TITLE:", "").strip()
              elif clean_line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean_line.replace("DESCRIPTION:", "").strip()
              elif clean_line.startswith("CATEGORY:"): parsed["CATEGORY"] = clean_line.replace("CATEGORY:", "").strip()
              elif clean_line.startswith("TAGS:"): parsed["TAGS"] = clean_line.replace("TAGS:", "").strip()
              elif clean_line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean_line.replace("IMAGE_KEYWORD:", "").strip()
              elif clean_line.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower())
          slug = re.sub(r'-+', '-', slug).strip('-')
          tag_list = [t.strip() for t in parsed["TAGS"].split(',')]

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_str}"
          author: "Jonathan Mwaniki"
          image: "{image_url}"
          imageCaption: "Image for {parsed['IMAGE_KEYWORD']}"
          imageAlt: "{parsed['IMAGE_KEYWORD']}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps(tag_list)}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Published:</strong> {date_str}</p>
            <p><strong>Source:</strong> <a href="https://twitter.com/{source_handle}">@{source_handle}</a></p>
          </div>
          """

          import textwrap
          final_file = textwrap.dedent(final_file).strip()
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          print(f"‚úÖ Published: {slug}.md")