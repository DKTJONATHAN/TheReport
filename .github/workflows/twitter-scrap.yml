name:  Newser

on:
  schedule:
    - cron: '0 23 * * *'  # 2AM EAT (Prev Day UTC)
    - cron: '0 5 * * *'   # 8AM EAT
    - cron: '0 10 * * *'  # 1PM EAT
    - cron: '0 13 * * *'  # 4PM EAT
    - cron: '0 17 * * *'  # 8PM EAT
    - cron: '0 21 * * *'  # 12AM EAT
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 google-genai

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time
          from bs4 import BeautifulSoup
          from google import genai
          from google.genai import types

          # --- 1. SEARCH NYAKUNDI REPORT ---
          def get_nyakundi_news():
              url = "https://html.duckduckgo.com/html/"
              params = {'q': 'site:nyakundireport.com', 'df': 'd'}
              headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36..."}
              print(f"ðŸ•·ï¸ Scanning nyakundireport.com for today's stories...")
              try:
                  resp = requests.get(url, params=params, headers=headers, timeout=15)
                  soup = BeautifulSoup(resp.text, "html.parser")
                  results = []
                  for res in soup.select(".result"):
                      title_tag = res.select_one(".result__a")
                      snippet_tag = res.select_one(".result__snippet")
                      if title_tag:
                          results.append({
                              "title": title_tag.get_text(strip=True),
                              "url": title_tag.get("href"),
                              "snippet": snippet_tag.get_text(strip=True) if snippet_tag else ""
                          })
                  return results
              except: return []

          # 1. Fetch latest
          news_data = get_nyakundi_news()
          if not news_data:
              print("âŒ No news found today on the site.")
              exit(0)

          target = news_data[0]
          print(f"ðŸ“º Top Story: {target['title']}")

          # --- 2. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://image.pollinations.ai/prompt/news-{}-kenya?width=1200&height=630".format(query.replace(" ", "-"))
              if not access_key: return fallback
              try:
                  url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
                  r = requests.get(url, timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          # --- 3. THE GEMINI CALL (WITH 429 RETRY) ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          model_id = "gemini-2.0-flash"

          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.
          SOURCE: {target['title']}
          SNIPPET: {target['snippet']}
          TASK: Write a 1200-1600 word news article. Expand on entities. Neutrally journalist tone.
          OUTPUT FORMAT:
          TITLE: [Headline]
          DESCRIPTION: [Summary]
          CATEGORY: [Category]
          TAGS: [Tags]
          IMAGE_KEYWORD: [Keywords]
          BODY: [Full text with ## Headers]
          '''

          full_text = ""
          for attempt in range(3):
              try:
                  print(f"ðŸ¤– Generating (Attempt {attempt+1})...")
                  response = client.models.generate_content(model=model_id, contents=prompt)
                  full_text = response.text.strip()
                  break
              except Exception as e:
                  if "429" in str(e):
                      print("âš ï¸ Quota hit. Waiting 20 seconds to retry...")
                      time.sleep(20)
                  else:
                      print(f"âŒ Error: {e}")
                      exit(1)

          if not full_text: exit(1)

          # --- 4. PARSE & SAVE ---
          parsed = { "TITLE": target['title'], "DESCRIPTION": target['snippet'][:160], "CATEGORY": "News", "TAGS": "news", "IMAGE_KEYWORD": "kenya", "BODY": "" }
          current_section = None
          for line in full_text.splitlines():
              clean = line.strip().replace("**", "")
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean.replace("DESCRIPTION:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean_line.replace("TAGS:", "").strip() if 'clean_line' in locals() else clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          date_now = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_now}"
          author: "Jonathan Mwaniki"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps([t.strip() for t in parsed.get('TAGS', 'news').split(',')])}
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Source:</strong> <a href="{target['url']}">The Nyakundi Report</a></p>
          </div>
          """
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          print(f"âœ… Published: {slug}.md")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish ${{ github.run_id }}"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Content Bot"
          commit_user_email: "actions@github.com"