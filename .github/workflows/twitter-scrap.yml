name: Auto Newser

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies (Playwright)
        run: |
          python -m pip install --upgrade pip
          # We need playwright to mimic a real user
          pip install requests google-genai playwright
          # Install the browser engine
          playwright install chromium --with-deps

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time
          from google import genai
          from google.genai import types
          from playwright.sync_api import sync_playwright

          # --- 1. CONFIGURATION ---
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour

          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['sports', 'technology', 'business']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              else: topic = 'business'

          print(f"üéØ Topic: {topic.upper()}")

          # --- 2. THE NUCLEAR OPTION: PLAYWRIGHT SCRAPER ---
          # We are searching Google News directly using a real browser engine.
          
          # Google News Search URL (Kenya Edition)
          # tbm=nws -> News Tab
          # gl=KE -> Country Kenya
          
          SEARCH_QUERIES = {
              'sports': 'Kenya Sports News Football Athletics',
              'technology': 'Kenya Technology News Startup Safaricom',
              'business': 'Kenya Business Finance Economy'
          }
          
          query_text = SEARCH_QUERIES.get(topic, 'Kenya News')
          url = f"https://www.google.com/search?q={query_text}&gl=KE&tbm=nws"

          print(f"üï∑Ô∏è Launching Headless Browser for: {url}")

          scraped_items = []

          with sync_playwright() as p:
              # Launch Chrome. headless=True means it runs invisibly.
              # We add arguments to hide the fact that it is a bot.
              browser = p.chromium.launch(headless=True, args=[
                  '--disable-blink-features=AutomationControlled',
                  '--no-sandbox',
                  '--disable-setuid-sandbox'
              ])
              
              # Create a context with a real User-Agent
              context = browser.new_context(
                  user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                  viewport={'width': 1280, 'height': 720}
              )
              
              page = context.new_page()
              
              try:
                  page.goto(url, timeout=30000)
                  # Wait for the search results container
                  page.wait_for_selector('div#search', state='attached')
                  
                  # Extract news cards (Google News uses specific classes like 'SoaBEf' or 'WlydMc')
                  # But the safest way is to look for the generic search result containers in the news tab
                  results = page.locator('div[data-snhf]').all()
                  
                  if not results:
                      # Fallback for different HTML structure
                      results = page.locator('div.SoaBEf').all()

                  print(f"üîé Found {len(results)} raw results on page.")

                  for res in results[:6]: # Check top 6
                      try:
                          # Extract Title
                          title_el = res.locator('div[role="heading"]').first
                          if not title_el.count(): continue
                          title = title_el.inner_text()
                          
                          # Extract Link
                          link_el = res.locator('a').first
                          link = link_el.get_attribute('href')
                          
                          # Extract Source (often in a span or div below title)
                          # This is "best effort" scraping
                          text_content = res.inner_text()
                          
                          # Basic filters
                          if len(title) > 20 and link and link.startswith('http'):
                              scraped_items.append({
                                  'title': title,
                                  'link': link,
                                  'raw': text_content
                              })
                      except: continue
                      
              except Exception as e:
                  print(f"‚ö†Ô∏è Browser Error: {e}")
              finally:
                  browser.close()

          if not scraped_items:
              print("‚ùå No news found (Google blocked the browser or layout changed).")
              exit(0)

          # --- 3. FILTERING ---
          selected_item = scraped_items[0]
          
          # Try to find a "Preferred" source if possible
          PREFERRED = ['Nation', 'Standard', 'Capital', 'Business Daily', 'Techweez', 'Mozzart']
          for item in scraped_items:
              if any(p.lower() in item['raw'].lower() for p in PREFERRED):
                  selected_item = item
                  break

          print(f"üì∫ Selected Story: {selected_item['title']}")
          
          # --- 4. DATA PREP ---
          headline = selected_item['title']
          link = selected_item['link']
          source_text = selected_item['raw'].replace('\n', ' ')[:50] # Snippet for context

          # --- 5. IMAGE HELPER (Unchanged) ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = f"https://image.pollinations.ai/prompt/journalistic-photo-of-{query.replace(' ', '-')}-news?width=1200&height=630&nologo=true"
              if not access_key: return fallback
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=5)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except: pass
              return fallback

          # --- 6. GEMINI CALL (Unchanged) ---
          hooks = [
             "Breaking Update: Here are the details on the developing story.",
             "Market Watch: Analyzing the latest shift in the landscape.",
             "Key Report: What this latest announcement means for Kenya."
          ]
          selected_hook = random.choice(hooks)

          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.

          NEWS SOURCE:
          Headline: "{headline}"
          Link: {link}
          Context: "{source_text}"
          
          TASK: Write a news article (600-800 words) based on this headline.
          - Use your internal knowledge to FILL IN DETAILS about this event.
          - Opening: {selected_hook}

          OUTPUT FORMAT (exact):
          TITLE: [News-style headline, max 60 chars]
          DESCRIPTION: [Summary, max 160 chars]
          CATEGORY: [{topic.capitalize()}]
          TAGS: [3-5 tags, comma separated]
          IMAGE_KEYWORD: [2-3 words for image]
          BODY:
          [Article body with ## Headers]
          '''

          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          model_id = "gemini-2.0-flash"

          print(f"ü§ñ Generating with {model_id}...")
          try:
              response = client.models.generate_content(
                  model=model_id,
                  contents=prompt,
                  config=types.GenerateContentConfig(temperature=0.1)
              )
              full_text = response.text.strip()
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              exit(1)

          # --- 7. PARSE & SAVE (Unchanged) ---
          parsed = { "TITLE": headline[:60], "DESCRIPTION": headline[:160], "CATEGORY": topic.capitalize(), "TAGS": f"{topic},news,kenya", "IMAGE_KEYWORD": topic, "BODY": "" }
          current_section = None

          for line in full_text.splitlines():
              clean_line = line.strip().replace("**", "")
              if clean_line.startswith("TITLE:"): parsed["TITLE"] = clean_line.replace("TITLE:", "").strip()
              elif clean_line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean_line.replace("DESCRIPTION:", "").strip()
              elif clean_line.startswith("CATEGORY:"): parsed["CATEGORY"] = clean_line.replace("CATEGORY:", "").strip()
              elif clean_line.startswith("TAGS:"): parsed["TAGS"] = clean_line.replace("TAGS:", "").strip()
              elif clean_line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean_line.replace("IMAGE_KEYWORD:", "").strip()
              elif clean_line.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower())
          slug = re.sub(r'-+', '-', slug).strip('-')
          tag_list = [t.strip() for t in parsed["TAGS"].split(',')]

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_str}"
          author: "Jonathan Mwaniki"
          image: "{image_url}"
          imageCaption: "Image for {parsed['IMAGE_KEYWORD']}"
          imageAlt: "{parsed['IMAGE_KEYWORD']}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps(tag_list)}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Published:</strong> {date_str}</p>
            <p><strong>Source:</strong> Google News / {headline}</p>
          </div>
          """

          import textwrap
          final_file = textwrap.dedent(final_file).strip()
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          print(f"‚úÖ Published: {slug}.md")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish ${{ github.run_id }}"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Auto Newser"
          commit_user_email: "actions@github.com"
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}