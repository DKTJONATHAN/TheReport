name: Nyakundi Report Scraper

on:
  schedule:
    - cron: '0 0,6,12,18 * * *' # Runs 4 times a day (3AM, 9AM, 3PM, 9PM EAT)
  workflow_dispatch:

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  scrape-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 google-genai

      - name: Scrape and Generate
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
        run: |
          python << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          import json
          import datetime
          import os
          import re
          import random
          from google import genai
          from google.genai import types

          # --- 1. RESEARCHED DDG SCRAPER (CONVERTS TO JSON) ---
          def get_nyakundi_data():
              # We use the HTML Lite endpoint to avoid JS/Cookie blocks
              url = "https://html.duckduckgo.com/html/"
              
              # df=d: Filter for past 24 hours
              params = {'q': 'site:nyakundireport.com', 'df': 'd'}
              
              headers = {
                  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                  "Referer": "https://duckduckgo.com/"
              }
              
              print(f"ðŸ•·ï¸ Requesting SERP for nyakundireport.com...")
              try:
                  response = requests.get(url, params=params, headers=headers, timeout=15)
                  response.raise_for_status()
                  
                  soup = BeautifulSoup(response.text, "html.parser")
                  results = []

                  # Extract results into structured JSON
                  # Classes based on researched 2026 DDG HTML layout
                  for res in soup.select(".result"):
                      title_tag = res.select_one(".result__a")
                      snippet_tag = res.select_one(".result__snippet")
                      
                      if title_tag:
                          results.append({
                              "title": title_tag.get_text(strip=True),
                              "url": title_tag.get("href"),
                              "snippet": snippet_tag.get_text(strip=True) if snippet_tag else "",
                              "scraped_at": datetime.datetime.now().isoformat()
                          })
                  
                  return results
              except Exception as e:
                  print(f"âŒ Scraper Failed: {e}")
                  return []

          # 1. Capture JSON
          news_list = get_nyakundi_data()

          if not news_list:
              print("âŒ No fresh articles found on nyakundireport.com today. Exiting.")
              exit(0)

          # 2. Pick the latest indexed story
          target = news_list[0]
          print(f"ðŸ“¦ Target JSON: {json.dumps(target, indent=2)}")

          # 3. Feed to Gemini
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          
          prompt = f"""STRICT JOURNALISM MODE.
          Review the following JSON search result from The Nyakundi Report:
          TITLE: {target['title']}
          SNIPPET: {target['snippet']}
          URL: {target['url']}

          TASK:
          1. Determine the news category (Politics, Crime, Tech, Gossip, Business, etc).
          2. Write a 600-word update. Expand on the entities and names mentioned using your knowledge.
          3. Tone: Fast-paced, insider, investigative.

          OUTPUT FORMAT (EXACT):
          TITLE: [Urgent Headline]
          DESCRIPTION: [Summary max 160 chars]
          CATEGORY: [Single Word Category]
          TAGS: [3-5 tags, comma separated]
          IMAGE_KEYWORD: [2-3 words for image search]
          BODY:
          [Article body with ## Headers]
          """

          print(f"ðŸ¤– Generating with Gemini 2.0...")
          response = client.models.generate_content(
              model="gemini-2.0-flash",
              contents=prompt,
              config=types.GenerateContentConfig(temperature=0.1)
          )
          full_text = response.text.strip()

          # --- 4. PARSE & SAVE ---
          parsed = {"TITLE": target['title'], "BODY": "", "CATEGORY": "News"}
          current_section = None
          for line in full_text.splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = line.replace("DESCRIPTION:", "").strip()
              elif line.startswith("CATEGORY:"): parsed["CATEGORY"] = line.replace("CATEGORY:", "").strip()
              elif line.startswith("TAGS:"): parsed["TAGS"] = line.replace("TAGS:", "").strip()
              elif line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = line.replace("IMAGE_KEYWORD:", "").strip()
              elif line.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          # Handle Image
          def get_img(kw):
              fallback = f"https://image.pollinations.ai/prompt/journalistic-news-{kw.replace(' ', '-')}-kenya?width=1200&height=630"
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              if not key: return fallback
              try:
                  r = requests.get(f"https://api.unsplash.com/photos/random?query={kw}&orientation=landscape&client_id={key}", timeout=5)
                  return r.json()['urls']['regular'] if r.status_code == 200 else fallback
              except: return fallback

          image_url = get_img(parsed.get('IMAGE_KEYWORD', 'Kenya'))
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          date_now = datetime.datetime.utcnow().strftime("%Y-%m-%d")

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed.get('DESCRIPTION', '').replace('"', "'")}"
          date: "{date_now}"
          author: "Jonathan Mwaniki"
          image: "{image_url}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps([t.strip() for t in parsed.get('TAGS', 'news').split(',')])}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Source:</strong> <a href="{target['url']}">The Nyakundi Report</a></p>
          </div>
          """

          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
              f.write(final_file)
          print(f"âœ… Published: {slug}.md")
          EOF

      - name: Commit and Push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: update from nyakundireport.com"
          file_pattern: 'src/content/posts/*.md'
          commit_user_name: "Auto Newser"
          commit_user_email: "actions@github.com"