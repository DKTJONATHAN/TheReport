name: Autonomous Real-Time Publisher (Apify + Groq)

on:
  schedule:
    - cron: '0 1,6,10,15 * * *' # 4 AM, 9 AM, 1 PM, 6 PM EAT
  workflow_dispatch:

permissions:
  contents: write

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests groq apify-client

      - name: Crawl Kenyans.co.ke and Generate Content
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          POSTS_DIR: src/content/posts
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re
          from groq import Groq
          from apify_client import ApifyClient

          # 1. SETUP & DYNAMIC TIME
          now = datetime.datetime.utcnow()
          date_str = now.strftime('%Y-%m-%d')
          human_date = now.strftime('%B %d, %Y')
          
          client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
          apify = ApifyClient(token=os.environ.get("APIFY_API_TOKEN"))

          # 2. APIFY CRAWLER (Targeting Kenyans.co.ke)
          print(f"[{human_date}] Crawling Kenyans.co.ke for the latest heat...")
          
          run_input = {
              "startUrls": [{"url": "https://www.kenyans.co.ke/news"}],
              "maxCrawlPages": 3,
              "crawlerType": "cheerio", # Fast, lightweight, bypasses most blocks
              "maxCrawlDepth": 1,
              "removeElementsCssSelector": "nav, footer, script, style, .sidebar, .ads"
          }

          try:
              # Run the Website Content Crawler Actor
              run = apify.actor("apify/website-content-crawler").call(run_input=run_input)
              
              # Fetch Markdown results from the dataset
              news_context = ""
              for item in apify.dataset(run["defaultDatasetId"]).iterate_items():
                  # We only take the markdown content to keep it clean for the LLM
                  title = item.get('metadata', {}).get('title', 'No Title')
                  content = item.get('markdown', '')[:2000] # Cap per article for context limits
                  news_context += f"\n--- STORY: {title} ---\n{content}\n"
                  
          except Exception as e:
              print(f"Apify Error: {e}")
              exit(1)

          if not news_context.strip():
              print("No fresh content found on Kenyans.co.ke. Exiting.")
              exit(1)

          # 3. SAVAGE CONTENT GENERATION (Llama 3.3 70B)
          # First pass for raw, fearless facts
          raw_prompt = f"""
          TODAY'S DATE: {human_date}
          CRAWLED DATA FROM KENYANS.CO.KE:
          {news_context}

          TASK: Write a 1500-word, high-SEO, fearless, and unapologetic article based on these events.
          
          RULES:
          1. REALITY: If the crawl shows a person is dead or out of power, mention it as a legacy or failure.
          2. STYLE: Be bold. Mention names. Use vulgar or rough language if it adds human impact. No 'polite' AI talk.
          3. HUMANIZATION: Use sentence fragments, heavy Sheng (slang), and strong bias. No 'pivotal' or 'tapestry'.
          4. SEO: Headlines must be competitive and click-worthy for 2026.
          
          FORMAT:
          TITLE: [Savage Headline]
          DESCRIPTION: [SEO Meta]
          BODY: [Full Content]
          """
          
          response = client.chat.completions.create(
              model="llama-3.3-70b-versatile",
              messages=[{"role": "user", "content": raw_prompt}],
              temperature=0.9
          )
          article = response.choices[0].message.content

          # 4. THE HUMANIZER TEST (Second Pass for the AI-Smell Removal)
          final_response = client.chat.completions.create(
              model="llama-3.3-70b-versatile",
              messages=[{"role": "user", "content": f"Rewrite this as a pissed-off Nairobi expert. Use more Sheng and messy sentence fragments. Make it visceral. Remove all AI structure.\\n\\n{article}"}],
              temperature=1.0
          )
          final_text = final_response.choices[0].message.content

          # 5. PARSING & SAVING
          title_match = re.search(r"TITLE:\s*(.*)", final_text)
          clean_title = title_match.group(1).strip().replace('"', '') if title_match else f"Kenyans-News-{date_str}"
          slug = re.sub(r'[^a-z0-9-]', '-', clean_title.lower()).strip('-')
          
          desc_match = re.search(r"DESCRIPTION:\s*(.*)", final_text)
          clean_desc = desc_match.group(1).strip().replace('"', '') if desc_match else ""
          
          body = final_text.split("BODY:")[-1].strip()

          final_md = (
              f"---\n"
              f"title: \"{clean_title}\"\n"
              f"description: \"{clean_desc}\"\n"
              f"date: \"{date_str}\"\n"
              f"featured: true\n"
              f"---\n\n"
              f"{body}"
          )
          
          posts_dir = os.environ.get('POSTS_DIR', 'src/content/posts')
          os.makedirs(posts_dir, exist_ok=True)
          with open(f"{posts_dir}/{slug}.md", "w", encoding="utf-8") as f:
              f.write(final_md)
          EOF

      - name: Commit and Push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: savage Kenyans.co.ke crawl update [${{ github.run_id }}]"