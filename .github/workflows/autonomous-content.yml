name: International News (Specialized Sources)

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'   # 5:00 AM EAT (Politics)
    - cron: '0 5 * * *'   # 8:00 AM EAT (Sports)
    - cron: '0 8 * * *'   # 11:00 AM EAT (Business)
    - cron: '0 11 * * *'  # 2:00 PM EAT (Tech)
    - cron: '0 14 * * *'  # 5:00 PM EAT (Sports)
    - cron: '0 18 * * *'  # 9:00 PM EAT (Entertainment)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai

      - name: Generate article
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, hashlib
          from google import genai
          from google.genai import types

          # --- 1. TOPIC & DOMAIN MAPPING ---
          source_map = {
              'politics': 'reuters.com,apnews.com,nytimes.com,cnn.com',
              'sports': 'espn.com,bleacherreport.com,skysports.com,nba.com',
              'business': 'bloomberg.com,wsj.com,ft.com,forbes.com',
              'technology': 'theverge.com,techcrunch.com,wired.com,engadget.com',
              'entertainment': 'variety.com,hollywoodreporter.com,rollingstone.com'
          }

          cron = os.environ.get('CRON_SCHEDULE', '')
          manual = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          cron_map = {'0 2 * * *': 'politics', '0 5 * * *': 'sports', '0 8 * * *': 'business', '0 11 * * *': 'technology', '0 14 * * *': 'sports', '0 18 * * *': 'entertainment'}
          topic = manual if manual else cron_map.get(cron, 'politics')
          domains = source_map.get(topic, 'reuters.com')

          # --- 2. IMAGE RETRIEVAL ---
          def get_image(kw, tp):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1504711432869-efd597cdd042"
              if not key: return fallback
              for q in [kw, tp, "world journalism"]:
                  try:
                      r = requests.get(f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={key}", timeout=10)
                      if r.status_code == 200: return r.json()['urls']['regular']
                  except: continue
              return fallback

          # --- 3. FETCH NEWS (WITH FALLBACK) ---
          news_key = os.environ.get("NEWSAPI_KEY")
          if not news_key:
              print("❌ Error: NEWSAPI_KEY is missing from Secrets!"); exit(1)

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          
          def fetch_articles(query_domains, query_topic):
              url = f"https://newsapi.org/v2/everything?domains={query_domains}&q={query_topic}&language=en&sortBy=publishedAt&apiKey={news_key}"
              try:
                  res = requests.get(url).json()
                  return res.get('articles', [])
              except: return []

          articles = fetch_articles(domains, topic)
          if not articles:
              print(f"⚠️ No niche articles for {topic}, trying general sources...")
              articles = fetch_articles("reuters.com,apnews.com,bbc.co.uk", topic)

          target_article = None
          final_hash = None
          for art in articles:
              u_hash = hashlib.md5(art.get('url', '').encode()).hexdigest()
              if u_hash not in memory and len(art.get('content', '')) > 60:
                  target_article = art
                  final_hash = u_hash
                  break

          if not target_article:
              print(f"❌ No new articles found for {topic}."); exit(0)

          # --- 4. GEMINI REWRITE ---
          prompt = f"""STRICT REPORTING MODE for jonathanmwaniki.co.ke. 
          UK English. No em-dashes. Min 1400 words.
          SOURCE: {target_article['title']} - {target_article['description']} - {target_article['content']}
          TASK: Research and write a full report.
          FORMAT:
          TITLE: [headline]
          DESCRIPTION: [summary]
          CATEGORY: [{topic.title()}]
          TAGS: [3 tags]
          IMAGE_KEYWORD: [2 words]
          BODY: [Full article with ## headers: Background, Detailed Analysis, Global Impact, Reactions, Outlook]"""

          client = genai.Client(api_key=os.environ.get('GEMINI_API_KEY'))
          resp = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
          
          # --- 5. PARSE & CLEAN ---
          res_text = resp.text.strip()
          parsed = {"TITLE": "Global Report", "DESC": "", "IMG": topic, "BODY": ""}
          section = None
          for line in res_text.splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("DESCRIPTION:"): parsed["DESC"] = line.replace("DESCRIPTION:", "").strip()
              elif line.startswith("IMAGE_KEYWORD:"): parsed["IMG"] = line.replace("IMAGE_KEYWORD:", "").strip()
              elif line.startswith("BODY:"): section = "BODY"
              elif section == "BODY": parsed["BODY"] += line + "\n"

          # Dash scrubbing for style compliance
          for k in ["TITLE", "DESC", "BODY"]:
              parsed[k] = parsed[k].replace("—", ", ").replace("–", ", ").replace("--", ", ")
              parsed[k] = re.sub(r',\s*,', ',', parsed[k])

          img_url = get_image(parsed["IMG"], topic)
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')

          final_md = f"""---
          title: "{parsed['TITLE']}"
          description: "{parsed['DESC']}"
          date: "{datetime.datetime.utcnow().date().strftime('%Y-%m-%d')}"
          author: "Jonathan Mwaniki"
          image: "{img_url}"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}
          """
          
          # --- 6. SAVE ---
          import textwrap
          out_dir = os.environ.get("POSTS_DIR")
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w") as f:
              f.write(textwrap.dedent(final_md).strip())
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"✅ Published: {slug}")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish specialized news via NewsAPI"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'