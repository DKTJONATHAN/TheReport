name: Auto kenya

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 4 * * *'   # 7AM EAT (Entertainment)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 20 * * *'  # 11PM EAT (Entertainment)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business, entertainment)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client

      - name: Generate article with Gemini
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          POSTS_DIR: src/content/posts
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, textwrap
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient

          # --- 1. DYNAMIC CONFIGURATION ---
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          
          # Logic to determine topic based on time
          if manual_input in ['sports', 'technology', 'business', 'politics', 'entertainment']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 4 or current_hour_utc == 20: topic = 'entertainment'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              elif current_hour_utc == 18: topic = 'finance'
              else: topic = 'business'
          
          # Site Mapping and CSS Selectors for the "First Article" link
          site_config = {
              'technology': {
                  'url': 'https://techmoran.com/',
                  'glob': 'https://techmoran.com/*',
                  'selector': 'h2.entry-title a, h3.entry-title a'
              },
              'sports': {
                  'url': 'https://www.pulsesports.co.ke/',
                  'glob': 'https://www.pulsesports.co.ke/*/story/*',
                  'selector': 'a[href*="/story/"]'
              },
              'business': {
                  'url': 'https://www.kenyamoja.com/business',
                  'glob': 'https://www.kenyamoja.com/business/*',
                  'selector': '.item-list a, .news-title a'
              },
              'entertainment': {
                  'url': 'https://www.mpasho.co.ke/',
                  'glob': 'https://www.mpasho.co.ke/*',
                  'selector': 'a.article-card__link, .latest-news a'
              },
              'politics': {
                  'url': 'https://www.kenyans.co.ke/news',
                  'glob': 'https://www.kenyans.co.ke/news/*',
                  'selector': 'h2 a, .news-post a'
              }
          }
          
          cfg = site_config.get(topic, site_config['politics'])
          print(f"üéØ Topic: {topic} | Target: {cfg['url']}")

          # --- 2. AUTONOMOUS DEEP CRAWL ---
          apify = ApifyClient(os.environ.get("APIFY_API_TOKEN"))
          crawl_text = ""
          try:
              # We instruct Apify to find the first link matching the selector and "follow" it
              run_input = {
                  "startUrls": [{"url": cfg['url']}],
                  "maxCrawlPages": 2, # Page 1: Home, Page 2: The actual Story
                  "maxCrawlDepth": 1,
                  "includeUrlGlobs": [{"glob": cfg['glob']}],
                  "crawlerType": "cheerio",
                  "removeElementsCssSelector": "nav, footer, script, style, .sidebar, .ad, .comments"
              }
              run = apify.actor("apify/website-content-crawler").call(run_input=run_input)
              
              # Extract the story content (usually the second item in the dataset)
              results = list(apify.dataset(run["defaultDatasetId"]).iterate_items())
              if len(results) > 1:
                  # Use the item with the longest text (which is the full article)
                  crawl_text = max([r.get('markdown', '') for r in results], key=len)
              else:
                  crawl_text = results[0].get('markdown', '')
                  
          except Exception as e:
              print(f"‚ùå Crawl Error: {e}")
              exit(0)

          if len(crawl_text) < 600:
              print("‚ö†Ô∏è Content too short. Likely failed to hit the full article.")
              exit(0)

          # --- 3. PROMPT & GENERATION ---
          client = genai.Client(api_key=os.environ.get("GEMINI_WRITE_KEY"))
          model_id = "gemini-2.5-flash"

          prompt = f'''STRICT JOURNALISM MODE. 
          SOURCE MATERIAL: {crawl_text}

          TASK: Write a full factual news article (1200+ words).
          RULES: NO CITATIONS [site.com]. NO URLS. NO FOOTERS. NO EM-DASHES. UK English.
          
          OUTPUT:
          TITLE: [Headline]
          DESCRIPTION: [Summary]
          CATEGORY: [{topic.title()}]
          TAGS: [3-5 tags]
          IMAGE_KEYWORD: [Image search term]
          BODY:
          [Full content with ## Headers]'''

          try:
              resp = client.models.generate_content(model=model_id, contents=prompt)
              full_text = resp.text.strip()
          except: exit(1)

          # --- 4. PARSE & CLEANUP ---
          parsed = {"TITLE":"", "BODY":""}
          body_started = False
          for line in full_text.splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("BODY:"): body_started = True
              elif body_started: parsed["BODY"] += line + "\n"

          # Final citation and dash removal
          parsed["BODY"] = re.sub(r'\[.*?\]', '', parsed["BODY"])
          parsed["BODY"] = parsed["BODY"].replace("‚Äî", ", ").replace("‚Äì", ", ")

          # --- 5. SAVE ---
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          date: "{date_str}"
          author: "Jonathan Mwaniki"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          {parsed['BODY'].strip()}
          """

          os.makedirs(os.environ.get("POSTS_DIR"), exist_ok=True)
          with open(os.path.join(os.environ.get("POSTS_DIR"), f"{slug}.md"), "w") as f:
              f.write(textwrap.dedent(final_file).strip())
          EOF

      - name: Commit and push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish news"
          branch: ${{ env.DEFAULT_BRANCH }}