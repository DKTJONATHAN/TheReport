name: Auto Newser

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 4 * * *'   # 7AM EAT (Entertainment)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 20 * * *'  # 11PM EAT (Entertainment)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business, politics, entertainment)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client

      - name: Generate article with Gemini
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time
          from google import genai
          from apify_client import ApifyClient

          # --- 1. CONFIG & ROTATION ---
          apify_tokens = [os.environ.get(f"APIFY_API_TOKEN{i}" if i > 0 else "APIFY_API_TOKEN") for i in range(5)]
          active_apify = [t for t in apify_tokens if t]
          selected_apify = random.choice(active_apify) if active_apify else None

          # Gemini Key Pool (Rotating through all 4 available)
          gemini_keys = [
              os.environ.get("GEMINI_API_KEY"),
              os.environ.get("GEMINI_API_KEY1"),
              os.environ.get("GEMINI_API_KEY2"),
              os.environ.get("GEMINI_WRITE_KEY")
          ]
          active_gemini = [k for k in gemini_keys if k]

          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          
          if manual_input in ['sports', 'technology', 'business', 'politics', 'entertainment']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 4 or current_hour_utc == 20: topic = 'entertainment'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              elif current_hour_utc == 11 or current_hour_utc == 18: topic = 'business'
              else: topic = 'politics'
          
          site_config = {
              'technology': {'url': 'https://techweez.com/', 'glob': 'https://techweez.com/*'},
              'sports': {'url': 'https://www.pulsesports.co.ke/', 'glob': 'https://www.pulsesports.co.ke/*/story/*'},
              'business': {'url': 'https://kenyanwallstreet.com/', 'glob': 'https://kenyanwallstreet.com/*'},
              'entertainment': {'url': 'https://buzzcentral.co.ke/', 'glob': 'https://buzzcentral.co.ke/*'},
              'politics': {'url': 'https://www.kenyans.co.ke/news', 'glob': 'https://www.kenyans.co.ke/news/*'}
          }
          cfg = site_config.get(topic, site_config['politics'])

          # --- 2. CRAWL ---
          if not selected_apify: exit(1)
          apify = ApifyClient(selected_apify)
          crawl_text = ""
          try:
              run = apify.actor("apify/website-content-crawler").call(run_input={
                  "startUrls": [{"url": cfg['url']}],
                  "maxCrawlPages": 3,
                  "maxCrawlDepth": 1,
                  "includeUrlGlobs": [{"glob": cfg['glob']}],
                  "crawlerType": "cheerio"
              })
              results = list(apify.dataset(run["defaultDatasetId"]).iterate_items())
              valid_md = [r.get('markdown', '') for r in results if r.get('markdown')]
              if not valid_md: exit(0)
              crawl_text = max(valid_md, key=len)
          except: exit(0)

          # --- 3. GENERATE (WITH KEY SWAPPING) ---
          if not active_gemini: exit(1)
          ai_out = ""
          prompt = f"SOURCE: {crawl_text}\nWrite a 1200+ word UK English news article. NO CITATIONS. NO EM-DASHES.\nFormat: TITLE:, DESCRIPTION:, TAGS:, IMAGE_KEYWORD:, BODY:"

          for key in active_gemini:
              try:
                  client = genai.Client(api_key=key)
                  resp = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
                  ai_out = resp.text.strip()
                  if ai_out: break
              except Exception as e:
                  print(f"Key failed, trying next... Error: {e}")
                  continue

          if not ai_out: exit(1)

          # --- 4. PARSE ---
          def get_field(label, text):
              match = re.search(rf'{label}:\s*(.*)', text, re.IGNORECASE)
              return match.group(1).strip() if match else ""

          title = get_field("TITLE", ai_out) or f"News {date_str}"
          desc = get_field("DESCRIPTION", ai_out)
          tags_raw = get_field("TAGS", ai_out)
          tags = [t.strip() for t in tags_raw.split(',')] if tags_raw else [topic]
          img_kw = get_field("IMAGE_KEYWORD", ai_out) or topic
          body = re.search(r'BODY:\s*([\s\S]*)', ai_out, re.IGNORECASE).group(1).strip()
          body = body.replace("—", ", ").replace("–", ", ").replace("--", ", ")

          # --- 5. IMAGE ---
          image_url = f"https://image.pollinations.ai/prompt/{img_kw.replace(' ', '-')}-news-photo?width=1200&height=630&nologo=true"

          # --- 6. SAVE ---
          slug = re.sub(r'[^a-z0-9-]', '-', title.lower()).strip('-')
          front_matter = {
              "title": title.replace('"', "'"),
              "description": desc.replace('"', "'"),
              "date": date_str,
              "author": "Jonathan Mwaniki",
              "image": image_url,
              "category": topic.title(),
              "tags": tags,
              "featured": True,
              "draft": False,
              "slug": slug
          }
          final_post = f"---\n{json.dumps(front_matter, indent=2)}\n---\n\n{body}"
          out_path = os.path.join(os.environ.get("POSTS_DIR"), f"{slug}.md")
          os.makedirs(os.path.dirname(out_path), exist_ok=True)
          with open(out_path, "w", encoding="utf-8") as f:
              f.write(final_post)
          EOF

      - name: Commit and push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish autonomous news article"
          branch: ${{ env.DEFAULT_BRANCH }}