name: Autonomous Real-Time Publisher

on:
  schedule:
    - cron: '0 1 * * *'   # 4 AM EAT: Big World Tech
    - cron: '0 6 * * *'   # 9 AM EAT: Sports
    - cron: '0 10 * * *'  # 1 PM EAT: Politics
    - cron: '0 15 * * *'  # 6 PM EAT: Entertainment
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Topic Override'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests groq apify-client

      - name: Generate Content
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time
          from groq import Groq
          from apify_client import ApifyClient

          # --- 1. TOPIC & DYNAMIC ANGLE LOGIC ---
          date_str = datetime.datetime.utcnow().strftime('%Y-%m-%d')
          current_hour_utc = datetime.datetime.utcnow().hour
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip()

          # Topic Mapping
          if manual_input:
              topic, focus = manual_input, "User defined manual topic."
          elif 0 <= current_hour_utc < 4:
              topic, focus = "Big World Technology", "Breakthroughs in AI, Space, and Quantum. Global focus."
          elif 4 <= current_hour_utc < 9:
              topic, focus = "Sports", "Kenyan Leagues (FKF-PL), European Leagues, and International Athletics."
          elif 9 <= current_hour_utc < 14:
              topic, focus = "Politics", "Major international shifts mixed with African leadership and policy."
          else:
              topic, focus = "Social & Entertainment", "World celebrities mixed with Kenyan social trends."

          # Original Hook System
          hooks = [
             "A Direct Question: Start immediately with a hard-hitting analytical question.",
             "A Shocking Statistic: Start with a complex figure and explain its deeper meaning.",
             "A Cynical Observation: Start by critically challenging a common global belief.",
             "A Bold Critique: Start by identifying a specific group or policy and analyzing its failure."
          ]
          selected_hook = random.choice(hooks)

          # --- 2. CONTEXT CRAWL (APIFY) ---
          crawl_context = ""
          try:
              apify = ApifyClient(os.environ.get("APIFY_API_TOKEN"))
              run_input = {
                  "startUrls": [{"url": "https://www.kenyans.co.ke/news"}],
                  "maxCrawlPages": 1, 
                  "crawlerType": "cheerio",
                  "removeElementsCssSelector": "nav, footer, script, style, .sidebar, .ads"
              }
              # Run the crawler
              run = apify.actor("apify/website-content-crawler").call(run_input=run_input)
              
              # Extract text
              for item in apify.dataset(run["defaultDatasetId"]).iterate_items():
                  title = item.get('metadata', {}).get('title', '')
                  text = item.get('markdown', '')[:800] # Limit per article to save tokens
                  crawl_context += f"NEWS ITEM: {title}\nDETAILS: {text}\n"
          except Exception as e:
              print(f"Apify Error: {e}")
              crawl_context = "No real-time context available. Rely on general knowledge."

          # --- 3. THE COMMAND PROMPT (GROQ) ---
          spec_text = f"""
          You are an autonomous senior correspondent for jonathanmwaniki.co.ke.
          TODAY'S DATE: {date_str}
          TOPIC: {topic} | FOCUS: {focus}
          OPENING STYLE: {selected_hook}
          CONTEXT DATA: {crawl_context[:4000]}

          CRITICAL INSTRUCTIONS:
          1. REAL-TIME ANALYSIS: Use the CONTEXT DATA to write about a real event if matches topic.
          2. LANGUAGE: STRICTLY British English (colour, honour). NO Swahili. NO Sheng.
          3. TONE: Cynical, fearless, and intellectually savage. Name names.
          4. ANTI-SPAM: NEVER use 'geopolitical', 'landscape', 'tapestry', 'delve', or 'pivotal'.
          5. DASH RULE: Use NO em dashes (—) or en dashes (–). Use commas.
          6. FORMAT: Write 1500 words. 6 sections with ## Headers.

          *** OUTPUT FORMAT ***
          TITLE: [Punchy Title]
          DESCRIPTION: [2-sentence SEO summary]
          CATEGORY: [Sports/Business/Technology/Politics/Social]
          TAGS: [tag1, tag2, tag3]
          IMAGE_KEYWORD: [Specific cinematic noun for Unsplash]
          BODY:
          [Full article content. NO title at start.]
          """

          # --- 4. API CALL (GROQ) ---
          client = Groq(api_key=os.environ.get("GROQ_API_KEY"))
          try:
              chat_completion = client.chat.completions.create(
                  messages=[{"role": "user", "content": spec_text}],
                  model="llama-3.3-70b-versatile",
                  temperature=0.7,
                  max_tokens=5000
              )
              full_text = chat_completion.choices[0].message.content
          except Exception as e:
              print(f"Groq Error: {e}")
              exit(1)

          if not full_text: exit(1)

          # --- 5. PARSING & SCRUBBING ---
          parsed = {"TITLE": "Post", "DESCRIPTION": "", "CATEGORY": "Opinion", "TAGS": "news", "IMAGE_KEYWORD": "Nairobi"}
          for line in full_text.splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = line.replace("DESCRIPTION:", "").strip()
              elif line.startswith("CATEGORY:"): parsed["CATEGORY"] = line.replace("CATEGORY:", "").strip()
              elif line.startswith("TAGS:"): parsed["TAGS"] = line.replace("TAGS:", "").strip()
              elif line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = line.replace("IMAGE_KEYWORD:", "").strip()

          if "BODY:" in full_text:
              body_start = full_text.find("BODY:") + 5
              raw_body = full_text[body_start:].strip()
          else:
              # Fallback if AI forgets BODY: tag
              raw_body = full_text

          def scrub(t): return t.replace("\u2014", ", ").replace("\u2013", ", ").replace("--", ", ")
          parsed["BODY"], parsed["TITLE"], parsed["DESCRIPTION"] = scrub(raw_body), scrub(parsed["TITLE"]), scrub(parsed["DESCRIPTION"])

          # --- 6. IMAGE GUARANTEE PROTOCOL ---
          # 1. Define a high-quality fallback (Abstract News Background)
          img_url = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?auto=format&fit=crop&w=1200&q=80"
          
          # 2. Try to get a real one
          img_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          if img_key:
              try:
                  query = parsed['IMAGE_KEYWORD'].replace(" ", "+")
                  s_url = f"https://api.unsplash.com/search/photos?query={query}&client_id={img_key}&per_page=1&orientation=landscape"
                  s_res = requests.get(s_url, timeout=10).json()
                  if s_res.get('results') and len(s_res['results']) > 0:
                      img_url = s_res['results'][0]['urls']['regular']
              except:
                  pass # Silently fail to the fallback

          # --- 7. FILE ASSEMBLY ---
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          tag_list = [t.strip() for t in parsed["TAGS"].split(',')]

          final_md = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_str}"
          author: "Jonathan Mwaniki"
          image: "{img_url}"
          imageCaption: "Image for {parsed['IMAGE_KEYWORD']}"
          imageAlt: "{parsed['IMAGE_KEYWORD']}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps(tag_list)}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Published:</strong> {date_str}</p>
            <p><strong>Author:</strong> Jonathan Mwaniki</p>
            <p><strong>Tags:</strong> {parsed['TAGS']}</p>
          </div>
          """

          out_dir = os.environ.get('POSTS_DIR', 'src/content/posts')
          os.makedirs(out_dir, exist_ok=True)
          with open(f"{out_dir}/{slug}.md", "w", encoding="utf-8") as f:
              f.write(final_md)
          EOF

      - name: Commit and Push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: autonomous real-time update [${{ github.run_id }}]"
          branch: main