name: Auto kenya

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 4 * * *'   # 7AM EAT (Entertainment)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 20 * * *'  # 11PM EAT (Entertainment)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests google-genai apify-client beautifulsoup4

      - name: Deep Scrape and Generate
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic || '' }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from bs4 import BeautifulSoup
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient

          # --- 1. SETUP ---
          memory_path = os.environ.get('MEMORY_FILE')
          posts_dir = os.environ.get('POSTS_DIR')
          os.makedirs(posts_dir, exist_ok=True)
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          cron = os.environ.get('CRON_SCHEDULE', 'manual')
          key_map = {'0 0 * * *': 'GEMINI_API_KEY', '0 4 * * *': 'GEMINI_API_KEY1', '0 6 * * *': 'GEMINI_API_KEY2', '0 11 * * *': 'GEMINI_WRITE_KEY'}
          gemini_key = os.environ.get(key_map.get(cron, 'GEMINI_WRITE_KEY'))
          apify_tokens = [os.environ.get(f"APIFY_API_TOKEN{i}" if i > 0 else "APIFY_API_TOKEN") for i in range(5)]
          apify_client = ApifyClient(random.choice([t for t in apify_tokens if t]))

          h = datetime.datetime.utcnow().hour
          topic = os.environ.get('MANUAL_TOPIC', '').lower() or ('sports' if h in [0, 14] else 'entertainment' if h in [4, 20] else 'technology' if h in [6, 22] else 'business' if h in [11, 18] else 'politics')
          km_url = f"https://www.kenyamoja.com/{'news' if topic == 'politics' else topic}"

          # --- 2. FIND LINK ---
          res = requests.get(km_url, headers={'User-Agent': 'Mozilla/5.0'})
          soup = BeautifulSoup(res.text, 'html.parser')
          news_items = soup.select('li.news-item')
          final_target_link = None
          
          for item in news_items:
              link_tag = item.select_one('.news-title a')
              if link_tag and 'href' in link_tag.attrs:
                  url = link_tag['href']
                  if "kenyamoja.com" not in url and url.startswith("http"):
                      link_hash = hashlib.md5(url.encode()).hexdigest()
                      if link_hash not in memory:
                          final_target_link = url
                          memory.append(link_hash)
                          break

          if not final_target_link:
              print("No new stories.")
              exit(0)

          # --- 3. DEEP SCRAPE (Fix: Lower word limit + Custom Selectors) ---
          print(f"Deep scraping: {final_target_link}")
          try:
              run = apify_client.actor("lukaskrivka/article-extractor-smart").call(run_input={
                  "articleUrls": [{"url": final_target_link}],
                  "minWords": 50,
                  "proxyConfiguration": {"useApifyProxy": True},
                  "extendOutputFunction": "($) => { return { fullText: $('.article-body, .post-content, article').text() }; }"
              })
              dataset = apify_client.dataset(run["defaultDatasetId"]).list_items().items
              if not dataset: exit(0)
              data = dataset[0]
              full_text = data.get('fullText') or data.get('content') or data.get('text', '')
              if len(full_text) < 150: 
                  print("Article too short.")
                  exit(0)
          except Exception as e:
              print(f"Scrape failed: {e}")
              exit(0)

          # --- 4. GEMINI GENERATE ---
          client = genai.Client(api_key=gemini_key)
          prompt = f"SOURCE ARTICLE:\n{full_text}\n\nTask: Write a 1200+ word UK English news article for jonathanmwaniki.co.ke. No em-dashes. No citations.\n\nFORMAT:\nTITLE: [Headline]\nDESCRIPTION: [160 char summary]\nTAGS: [tag1, tag2]\nIMAGE_KEYWORD: [2-word search term]\nBODY: [Markdown Content]"
          
          response = client.models.generate_content(model="gemini-2.0-flash-exp", contents=prompt)
          ai_out = response.text.strip()

          # --- 5. PARSE & SAVE ---
          def get_field(label, text):
              match = re.search(rf'{label}:\s*(.*)', text, re.IGNORECASE)
              return match.group(1).strip() if match else ""

          title = get_field("TITLE", ai_out)
          desc = get_field("DESCRIPTION", ai_out)
          tags = get_field("TAGS", ai_out)
          img_kw = get_field("IMAGE_KEYWORD", ai_out)
          body = re.search(r'BODY:\s*([\s\S]*)', ai_out, re.IGNORECASE).group(1).strip().replace("â€”", ", ")
          slug = re.sub(r'[^a-z0-9-]', '-', title.lower()).strip('-')
          
          final_post = f"---\ntitle: \"{title}\"\ndescription: \"{desc}\"\ndate: \"{datetime.datetime.utcnow().strftime('%Y-%m-%d')}\"\nauthor: \"Jonathan Mwaniki\"\nimage: \"https://image.pollinations.ai/prompt/{img_kw.replace(' ', '-')}-news-photo?width=1200&height=630&nologo=true\"\ncategory: \"{topic.title()}\"\ntags: {json.dumps([t.strip() for t in tags.split(',')])}\nslug: \"{slug}\"\n---\n\n{body}"

          with open(os.path.join(posts_dir, f"{slug}.md"), "w") as f: f.write(final_post)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"Published: {title}")
          EOF

      - name: Commit and push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: deep scrape unique news article"
          branch: ${{ env.DEFAULT_BRANCH }}