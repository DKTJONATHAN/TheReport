name: International News

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'   # 5:00 AM EAT (Politics)
    - cron: '0 5 * * *'   # 8:00 AM EAT (Sports)
    - cron: '0 8 * * *'   # 11:00 AM EAT (Business)
    - cron: '0 11 * * *'  # 2:00 PM EAT (Tech)
    - cron: '0 14 * * *'  # 5:00 PM EAT (Sports)
    - cron: '0 18 * * *'  # 9:00 PM EAT (Entertainment)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Generate article with Gemini
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          APIFY_API_TOKEN5: ${{ secrets.APIFY_API_TOKEN5 }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient

          # --- 1. CONFIG & KEYS ---
          cron = os.environ.get('CRON_SCHEDULE', '')
          key_map = {'0 2 * * *': 'GEMINI_API_KEY', '0 5 * * *': 'GEMINI_API_KEY1', '0 8 * * *': 'GEMINI_API_KEY2', '0 11 * * *': 'GEMINI_API_KEY', '0 14 * * *': 'GEMINI_API_KEY1', '0 18 * * *': 'GEMINI_WRITE_KEY'}
          gemini_key = os.environ.get(key_map.get(cron, 'GEMINI_WRITE_KEY')) or os.environ.get('GEMINI_WRITE_KEY')
          
          apify_pool = [os.environ.get(f'APIFY_API_TOKEN{i}' if i > 0 else 'APIFY_API_TOKEN') for i in range(6)]
          apify_client = ApifyClient(random.choice([t for t in apify_pool if t]))

          # --- 2. TOPIC & TARGET ---
          cnn_sections = {'politics': 'https://edition.cnn.com/politics', 'sports': 'https://edition.cnn.com/sport', 'business': 'https://edition.cnn.com/business', 'technology': 'https://edition.cnn.com/business/tech', 'entertainment': 'https://edition.cnn.com/entertainment'}
          current_hour = datetime.datetime.utcnow().hour
          manual = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          topic = manual if manual in cnn_sections else 'politics'
          if not manual:
              if current_hour == 2: topic = 'politics'
              elif current_hour in [5, 14]: topic = 'sports'
              elif current_hour == 8: topic = 'business'
              elif current_hour == 11: topic = 'technology'
              elif current_hour == 18: topic = 'entertainment'

          # --- 3. IMAGE FALLBACK (STRICT) ---
          def get_image(kw, tp):
              key = os.environ.get("UNSPLASH_ACCESS_KEY")
              fallback = "https://images.unsplash.com/photo-1504711432869-efd597cdd042"
              if not key: return fallback
              for q in [kw, tp, "world news"]:
                  try:
                      r = requests.get(f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={key}", timeout=10)
                      if r.status_code == 200: return r.json()['urls']['regular']
                  except: continue
              return fallback

          # --- 4. DISCOVERY & SCRAPE ---
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          full_content, final_hash, final_url = None, None, None

          print(f"Searching for news in: {topic}")
          try:
              # Using a simpler discovery: crawl the section but don't try to click everything
              run = apify_client.actor("apify/website-content-crawler").call(run_input={
                  "startUrls": [{"url": cnn_sections[topic]}],
                  "maxCrawlPages": 1,
                  "crawlerType": "playwright:firefox",
                  "initialConcurrency": 1,
                  "maxCrawlDepth": 1,
                  "globs": [{"glob": "https://edition.cnn.com/2026/*/*/*/*.html"}]
              })
              
              for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
                  url = item.get("url", "")
                  u_hash = hashlib.md5(url.encode()).hexdigest()
                  if "/2026/" in url and u_hash not in memory:
                      # Deep scrape the specific article
                      print(f"Scraping article: {url}")
                      detail = apify_client.actor("apify/website-content-crawler").call(run_input={
                          "startUrls": [{"url": url}],
                          "maxCrawlPages": 1,
                          "crawlerType": "playwright:firefox",
                          "removeElementsCssSelector": "nav, footer, script, style, .ad-slot, .featured-video, .article__social-share"
                      })
                      d_items = list(apify_client.dataset(detail["defaultDatasetId"]).iterate_items())
                      if d_items and len(d_items[0].get('markdown', '')) > 1200:
                          full_content = d_items[0]['markdown']
                          final_hash, final_url = u_hash, url
                          break
          except Exception as e:
              print(f"Scrape Error: {e}")

          if not full_content:
              print("No new content found."); exit(0)

          # --- 5. REWRITE & SAVE ---
          prompt = f"STRICT REPORTING. UK English. No em-dashes. Min 1350 words. SOURCE: {full_content[:15000]} OUTPUT: TITLE: [headline] DESCRIPTION: [summary] CATEGORY: [{topic.title()}] TAGS: [3 tags] IMAGE_KEYWORD: [2 words] BODY: [article with ## headers]"
          client = genai.Client(api_key=gemini_key)
          resp = client.models.generate_content(model="gemini-3-flash-preview", contents=prompt, config=types.GenerateContentConfig(temperature=0.1, max_output_tokens=8192))
          
          parsed = {"TITLE": "International News", "DESC": "", "IMG": topic, "BODY": ""}
          section = None
          for line in resp.text.strip().splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("DESCRIPTION:"): parsed["DESC"] = line.replace("DESCRIPTION:", "").strip()
              elif line.startswith("IMAGE_KEYWORD:"): parsed["IMG"] = line.replace("IMAGE_KEYWORD:", "").strip()
              elif line.startswith("BODY:"): section = "BODY"
              elif section == "BODY": parsed["BODY"] += line + "\n"

          # Final cleaning of dashes
          for k in ["TITLE", "DESC", "BODY"]:
              parsed[k] = parsed[k].replace("—", ", ").replace("–", ", ").replace("--", ", ")

          img_url = get_image(parsed["IMG"], topic)
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          
          final_md = f"""---
          title: "{parsed['TITLE']}"
          description: "{parsed['DESC']}"
          date: "{datetime.datetime.utcnow().date().strftime('%Y-%m-%d')}"
          author: "Jonathan Mwaniki"
          image: "{img_url}"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}
          """
          
          import textwrap
          out_dir = os.environ.get("POSTS_DIR")
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w") as f:
              f.write(textwrap.dedent(final_md).strip())
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish international report"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'