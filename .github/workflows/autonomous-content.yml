name: Auto Newser

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 4 * * *'   # 7AM EAT (Entertainment)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 20 * * *'  # 11PM EAT (Entertainment)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business, politics, entertainment)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client

      - name: Generate article with Gemini
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random
          from google import genai
          from apify_client import ApifyClient

          # --- 1. CONFIG & ROTATION ---
          apify_tokens = [os.environ.get(f"APIFY_API_TOKEN{i}" if i > 0 else "APIFY_API_TOKEN") for i in range(5)]
          active_apify = [t for t in apify_tokens if t]
          selected_apify = random.choice(active_apify) if active_apify else None

          gemini_keys = [os.environ.get("GEMINI_API_KEY"), os.environ.get("GEMINI_WRITE_KEY")]
          active_gemini = [k for k in gemini_keys if k]
          selected_gemini = random.choice(active_gemini) if active_gemini else None

          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          
          if manual_input in ['sports', 'technology', 'business', 'politics', 'entertainment']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 4 or current_hour_utc == 20: topic = 'entertainment'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              elif current_hour_utc == 11 or current_hour_utc == 18: topic = 'business'
              else: topic = 'politics'
          
          site_config = {
              'technology': {'url': 'https://techweez.com/', 'glob': 'https://techweez.com/*'},
              'sports': {'url': 'https://www.pulsesports.co.ke/', 'glob': 'https://www.pulsesports.co.ke/*/story/*'},
              'business': {'url': 'https://kenyanwallstreet.com/', 'glob': 'https://kenyanwallstreet.com/*'},
              'entertainment': {'url': 'https://buzzcentral.co.ke/', 'glob': 'https://buzzcentral.co.ke/*'},
              'politics': {'url': 'https://www.kenyans.co.ke/news', 'glob': 'https://www.kenyans.co.ke/news/*'}
          }
          cfg = site_config.get(topic, site_config['politics'])

          # --- 2. CRAWL ---
          if not selected_apify: exit(1)
          apify = ApifyClient(selected_apify)
          crawl_text = ""
          try:
              run = apify.actor("apify/website-content-crawler").call(run_input={
                  "startUrls": [{"url": cfg['url']}],
                  "maxCrawlPages": 3,
                  "maxCrawlDepth": 1,
                  "includeUrlGlobs": [{"glob": cfg['glob']}],
                  "crawlerType": "cheerio"
              })
              results = list(apify.dataset(run["defaultDatasetId"]).iterate_items())
              valid_md = [r.get('markdown', '') for r in results if r.get('markdown')]
              if not valid_md: exit(0)
              crawl_text = max(valid_md, key=len)
          except: exit(0)

          # --- 3. GENERATE ---
          if not selected_gemini: exit(1)
          client = genai.Client(api_key=selected_gemini)
          prompt = f"""SOURCE: {crawl_text}
          Write a 1200+ word UK English news article. NO [site.com] CITATIONS. NO EM-DASHES.
          Format precisely:
          TITLE: [Headline]
          DESCRIPTION: [160 char summary]
          TAGS: [tag1, tag2, tag3]
          IMAGE_KEYWORD: [2-word search term]
          BODY: [Full Markdown Content]"""

          try:
              # FIXED: Using gemini-2.0-flash
              resp = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
              ai_out = resp.text.strip()
          except Exception as e:
              print(f"Gemini Error: {e}")
              exit(1)

          # --- 4. PARSE ---
          def get_field(label, text):
              match = re.search(rf'{label}:\s*(.*)', text, re.IGNORECASE)
              return match.group(1).strip() if match else ""

          title = get_field("TITLE", ai_out) or f"News {date_str}"
          desc = get_field("DESCRIPTION", ai_out)
          tags_raw = get_field("TAGS", ai_out)
          tags = [t.strip() for t in tags_raw.split(',')] if tags_raw else [topic]
          img_kw = get_field("IMAGE_KEYWORD", ai_out) or topic
          
          body_match = re.search(r'BODY:\s*([\s\S]*)', ai_out, re.IGNORECASE)
          body = body_match.group(1).strip() if body_match else "Content unavailable."

          body = re.sub(r'\[.*?\]', '', body)
          body = body.replace("—", ", ").replace("–", ", ").replace("--", ", ")

          # --- 5. IMAGE ---
          def fetch_image(q):
              access = os.environ.get("UNSPLASH_ACCESS_KEY")
              if access:
                  url = f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={access}"
                  try:
                      r = requests.get(url, timeout=5)
                      if r.status_code == 200: return r.json()['urls']['regular']
                  except: pass
              return f"https://image.pollinations.ai/prompt/{q.replace(' ', '-')}-news-photo?width=1200&height=630&nologo=true"

          image_url = fetch_image(img_kw)

          # --- 6. SAVE ---
          slug = re.sub(r'[^a-z0-9-]', '-', title.lower()).strip('-')
          front_matter = {
              "title": title.replace('"', "'"),
              "description": desc.replace('"', "'"),
              "date": date_str,
              "author": "Jonathan Mwaniki",
              "image": image_url,
              "imageCaption": f"Latest update on {img_kw}",
              "imageAlt": f"News image for {title}",
              "category": topic.title(),
              "tags": tags,
              "featured": True,
              "draft": False,
              "slug": slug
          }
          final_post = f"---\n{json.dumps(front_matter, indent=2)}\n---\n\n{body}"
          out_path = os.path.join(os.environ.get("POSTS_DIR"), f"{slug}.md")
          os.makedirs(os.path.dirname(out_path), exist_ok=True)
          with open(out_path, "w", encoding="utf-8") as f:
              f.write(final_post)
          EOF

      - name: Commit and push
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish autonomous news article"
          branch: ${{ env.DEFAULT_BRANCH }}