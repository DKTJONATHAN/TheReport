- name: Scrape and Report
        env:
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic || '' }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from bs4 import BeautifulSoup
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient

          # --- 1. SETUP & MEMORY ---
          memory_path = os.environ.get('MEMORY_FILE')
          posts_dir = os.environ.get('POSTS_DIR')
          os.makedirs(posts_dir, exist_ok=True)
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          apify_client = ApifyClient(os.environ.get('APIFY_API_TOKEN'))
          gemini_client = genai.Client(api_key=os.environ.get('GEMINI_WRITE_KEY'))

          h = datetime.datetime.utcnow().hour
          topic = os.environ.get('MANUAL_TOPIC', '').lower() or ('sports' if h in [0, 14] else 'entertainment' if h in [4, 20] else 'technology' if h in [6, 22] else 'business' if h in [11, 18] else 'politics')
          km_url = f"https://www.kenyamoja.com/{'news' if topic == 'politics' else topic}"

          # --- 2. FIND EXTERNAL LINK ---
          res = requests.get(km_url, headers={'User-Agent': 'Mozilla/5.0'})
          soup = BeautifulSoup(res.text, 'html.parser')
          news_items = soup.select('li.news-item')
          final_target_link = None
          
          for item in news_items:
              link_tag = item.select_one('.news-title a')
              if link_tag and 'href' in link_tag.attrs:
                  url = link_tag['href']
                  if "kenyamoja.com" not in url and url.startswith("http"):
                      url_hash = hashlib.md5(url.encode()).hexdigest()
                      if url_hash not in memory:
                          final_target_link = url
                          memory.append(url_hash)
                          break

          if not final_target_link:
              print("No new external links found.")
              exit(0)

          # --- 3. SCRAPE CONTENT (APIFY) ---
          try:
              run = apify_client.actor("apify/website-content-crawler").call(run_input={
                  "startUrls": [{"url": final_target_link}],
                  "maxCrawlPages": 1,
                  "maxCrawlDepth": 0,
                  "crawlerType": "playwright:firefox",
                  "removeElementsCssSelector": "nav, footer, script, style, .ads, header"
              })
              data = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())[0]
              full_raw_content = data.get('markdown', data.get('text', ''))
          except Exception as e:
              print(f"Scrape failed: {e}"); exit(0)

          # --- 4. IMAGE HELPER (UNSPLASH ONLY) ---
          def get_unsplash_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              if not access_key:
                  return ""
              # Adding 'Kenya' to the query ensures localized results
              search_query = f"{query} Kenya"
              url = f"https://api.unsplash.com/photos/random?query={search_query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except:
                  pass
              return ""

          # --- 5. PROMPT (ENFORCING CONCRETE KEYWORDS) ---
          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.
          SOURCE CONTENT:
          {full_raw_content}

          TASK: Write a FACTUAL news article based on the source above. 
          - Tone: Neutral, professional journalist. 
          
          OUTPUT FORMAT:
          TITLE: [Headline]
          DESCRIPTION: [160 char summary]
          CATEGORY: [{topic.title()}]
          TAGS: [tag1, tag2, tag3]
          IMAGE_KEYWORD: [2-3 CONCRETE NOUNS for Unsplash search, e.g., "Nairobi Office" or "Football Stadium"]
          BODY:
          [1200-1600 words. ## Headers. No em-dashes.]

          RULES:
          - UK English only.
          - NO em-dashes (—) or en-dashes (–). Use commas.
          - Cite the source of information within the text.'''

          # --- 6. GENERATION ---
          response = gemini_client.models.generate_content(
              model="gemini-3-flash-preview",
              contents=prompt,
              config=types.GenerateContentConfig(temperature=0.1)
          )
          ai_out = response.text.strip()

          # --- 7. PARSE & SCRUB ---
          parsed = {"TITLE": "", "DESCRIPTION": "", "CATEGORY": topic.title(), "TAGS": "", "IMAGE_KEYWORD": topic, "BODY": ""}
          current_section = None
          for line in ai_out.splitlines():
              clean = line.strip().replace("**", "").replace('"', '')
              if clean.startswith("TITLE:"): parsed["TITLE"] = clean.replace("TITLE:", "").strip()
              elif clean.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean.replace("DESCRIPTION:", "").strip()
              elif clean.startswith("CATEGORY:"): parsed["CATEGORY"] = clean.replace("CATEGORY:", "").strip()
              elif clean.startswith("TAGS:"): parsed["TAGS"] = clean.replace("TAGS:", "").strip()
              elif clean.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean.replace("IMAGE_KEYWORD:", "").strip()
              elif clean.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          for key in ["TITLE", "DESCRIPTION", "BODY"]:
              parsed[key] = parsed[key].replace("—", ", ").replace("–", ", ").replace("--", ", ")

          # --- 8. SAVE ---
          image_url = get_unsplash_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          
          final_md = f"---\ntitle: \"{parsed['TITLE']}\"\ndescription: \"{parsed['DESCRIPTION']}\"\ndate: \"{date_str}\"\nauthor: \"Jonathan Mwaniki\"\nimage: \"{image_url}\"\ncategory: \"{parsed['CATEGORY']}\"\ntags: {json.dumps([t.strip() for t in parsed['TAGS'].split(',')])}\nslug: \"{slug}\"\n---\n\n# {parsed['TITLE']}\n\n{parsed['BODY']}\n\n<div class=\"source-credit\">Source: {final_target_link}</div>"
          
          with open(os.path.join(posts_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
              f.write(final_md)
          with open(memory_path, 'w') as f:
              json.dump(memory[-200:], f)
          print(f"Published: {slug}")
          EOF