name: International News

on:
  schedule:
    - cron: '0 2 * * *'   # 5:00 AM EAT (Politics)      -> KEY
    - cron: '0 5 * * *'   # 8:00 AM EAT (Sports)        -> KEY1
    - cron: '0 8 * * *'   # 11:00 AM EAT (Business)     -> KEY2
    - cron: '0 11 * * *'  # 2:00 PM EAT (Tech)          -> KEY
    - cron: '0 14 * * *'  # 5:00 PM EAT (Sports)        -> KEY1
    - cron: '0 18 * * *'  # 9:00 PM EAT (Entertainment) -> WRITE_KEY
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Generate article with Gemini
        env:
          # --- GEMINI KEYS ---
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          
          # --- APIFY TOKEN POOL ---
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          APIFY_API_TOKEN5: ${{ secrets.APIFY_API_TOKEN5 }}

          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup

          # --- 1. KEY ROTATION (GEMINI) ---
          cron = os.environ.get('CRON_SCHEDULE', '')
          key_map = {
              '0 2 * * *': 'GEMINI_API_KEY',
              '0 5 * * *': 'GEMINI_API_KEY1', 
              '0 8 * * *': 'GEMINI_API_KEY2',
              '0 11 * * *': 'GEMINI_API_KEY',
              '0 14 * * *': 'GEMINI_API_KEY1',
              '0 18 * * *': 'GEMINI_WRITE_KEY'
          }
          target_key_name = key_map.get(cron, 'GEMINI_WRITE_KEY')
          gemini_key = os.environ.get(target_key_name) or os.environ.get('GEMINI_WRITE_KEY')
          
          print(f"üîë Gemini Key: {target_key_name}")

          # --- 2. KEY ROTATION (APIFY) ---
          apify_pool = []
          for var in ['APIFY_API_TOKEN', 'APIFY_API_TOKEN1', 'APIFY_API_TOKEN2', 'APIFY_API_TOKEN3', 'APIFY_API_TOKEN4', 'APIFY_API_TOKEN5']:
              token = os.environ.get(var)
              if token: apify_pool.append(token)
          
          if not apify_pool:
              print("‚ùå No Apify tokens found!")
              exit(1)
              
          selected_apify_token = random.choice(apify_pool)
          masked_token = selected_apify_token[:5] + "..." + selected_apify_token[-5:]
          print(f"üï∑Ô∏è Apify Token: Selected from pool of {len(apify_pool)} (Using: {masked_token})")
          
          apify_client = ApifyClient(selected_apify_token)

          # --- 3. TOPIC SELECTION ---
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour
          
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['sports', 'technology', 'business', 'politics', 'entertainment']:
              topic = manual_input
          else:
              if current_hour_utc == 2: topic = 'politics'         # 5 AM EAT
              elif current_hour_utc == 5: topic = 'sports'         # 8 AM EAT
              elif current_hour_utc == 8: topic = 'business'       # 11 AM EAT
              elif current_hour_utc == 11: topic = 'technology'    # 2 PM EAT
              elif current_hour_utc == 14: topic = 'sports'        # 5 PM EAT
              elif current_hour_utc == 18: topic = 'entertainment' # 9 PM EAT
              else: topic = 'politics'
          
          print(f"üéØ Topic: {topic}")

          # --- 4. FETCH CANDIDATES (APIFY GOOGLE NEWS) ---
          # We use Apify to bypass the Google Login Wall that blocked 'requests'
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          print(f"üì° Fetching List via Apify (apify/google-news-scraper)...")
          
          try:
              # This actor mimics a real user on news.google.com
              list_run = apify_client.actor("apify/google-news-scraper").call(run_input={
                  "query": topic,
                  "maxItems": 10,
                  "language": "en-US",
                  "gl": "US",
                  "proxyConfiguration": { "useApifyProxy": True }
              })
              
              dataset_items = list(apify_client.dataset(list_run["defaultDatasetId"]).iterate_items())
          except Exception as e:
              print(f"‚ùå List Fetch Error: {e}")
              exit(1)

          candidates = []
          print(f"üîé Found {len(dataset_items)} raw items. Filtering...")

          for item in dataset_items:
              title = item.get("title", "")
              link = item.get("link", "")
              
              if link and len(title) > 15:
                  unique_key = f"{title}-{topic}"
                  url_hash = hashlib.md5(unique_key.encode()).hexdigest()
                  
                  if url_hash not in memory:
                      if not any(c['hash'] == url_hash for c in candidates):
                          candidates.append({
                              "url": link,
                              "hash": url_hash,
                              "title": title
                          })

          print(f"‚úÖ Found {len(candidates)} valid candidates.")
          if not candidates:
              print("‚ùå No new candidates found (all duplicates or empty).")
              exit(0)

          # --- 5. LINK UNMASKING HELPER ---
          # Even Apify might return the google redirect link (news.google.com/articles/...)
          # We must unmask it to scrape the real site.
          def unmask_google_link(google_url):
              if "news.google.com" not in google_url and "google.com" not in google_url:
                  return google_url # Already real

              print(f"  > üïµÔ∏è Unmasking: {google_url}")
              try:
                  # Use simple requests with headers to follow redirect
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                  }
                  resp = requests.head(google_url, allow_redirects=True, headers=headers, timeout=10)
                  
                  # Fallback to GET if HEAD fails
                  if resp.status_code >= 400:
                      resp = requests.get(google_url, allow_redirects=True, headers=headers, timeout=10)

                  final_url = resp.url
                  if "google.com" not in final_url:
                      return final_url
                  return None
              except Exception as e:
                  print(f"  > ‚ö†Ô∏è Unmask failed: {e}")
                  return None

          # --- 6. SCRAPE LOOP ---
          full_raw_content = None
          final_url = None
          final_hash = None

          # Process top 3 candidates
          for cand in candidates[:3]:
              print(f"\nProcessing: {cand['title'][:40]}...")
              
              # STEP 1: RESOLVE URL
              real_url = unmask_google_link(cand['url'])
              
              if not real_url:
                  print("  > ‚ùå Could not resolve link. Skipping.")
                  continue
                  
              print(f"  > üü¢ Target: {real_url}")
              
              # STEP 2: SCRAPE REAL URL
              try:
                  run = apify_client.actor("apify/website-content-crawler").call(run_input={
                      "startUrls": [{"url": real_url}],
                      "maxCrawlPages": 1,
                      "maxCrawlDepth": 0,
                      "crawlerType": "playwright:firefox",
                      "removeElementsCssSelector": "nav, footer, script, style, .ads, header, .cookie-banner, .popup",
                      "proxyConfiguration": {"useApifyProxy": True}
                  })
                  
                  items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
                  if items:
                      data = items[0]
                      text = data.get('markdown', data.get('text', ''))
                      
                      if text and len(text) > 800:
                          full_raw_content = text
                          final_url = real_url
                          final_hash = cand['hash']
                          print("  > ‚úÖ Scrape successful!")
                          break
                  print("  > ‚ö†Ô∏è Scrape content too short or empty.")
              except Exception as e:
                  print(f"  > ‚ùå Scrape error: {e}")
                  continue
          
          if not full_raw_content:
              print("‚ùå All candidates failed to scrape.")
              exit(0)

          # --- 7. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              if not access_key: return "" 
              
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except: pass
              return ""

          # --- 8. PROMPT ---
          hooks = [
             "Straight reporting: What happened, when, where, who, why.",
             "Key facts first: Timeline of events.",
             "Detailed analysis: Numbers and impacts."
          ]
          selected_hook = random.choice(hooks)

          cat_map = { 'sports':'Sports', 'technology':'Technology', 'business':'Business', 'politics':'Politics', 'entertainment':'Entertainment' }
          display_category = cat_map.get(topic, 'General')

          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.

          SOURCE CONTENT:
          {full_raw_content[:15000]}

          TASK: Write FACTUAL international news article. NO OPINIONS. NO HYPOTHESIS.
          - Write as the primary and only reporter.
          - NO mentions of "Original story by" or "Source:". 
          - Report in neutral, professional journalist tone.
          - Opening: {selected_hook}

          OUTPUT FORMAT (exact):
          TITLE: [News-style headline, 60 chars]
          DESCRIPTION: [Factual 1-2 sentence summary, 160 chars]
          CATEGORY: [{display_category}]
          TAGS: [3-5 relevant tags]
          IMAGE_KEYWORD: [2-3 words for image]
          BODY:
          [Full article: 1350-1800 words. ## Headers for sections. No em-dashes.]

          RULES:
          - Facts only. If unknown, say "details unclear".
          - No changing topic. Cover THIS story only.
          - UK English, commas not em-dashes.
          - 5-7 sections: Background, Key Developments, Impacts, Reactions, Next Steps.'''

          # --- 9. GEMINI CALL ---
          client = genai.Client(api_key=gemini_key)
          
          print(f"ü§ñ Generating with gemini-3-flash-preview...")
          
          try:
              response = client.models.generate_content(
                  model="gemini-3-flash-preview",
                  contents=prompt,
                  config=types.GenerateContentConfig(
                      temperature=0.1,
                      max_output_tokens=8192
                  )
              )
              full_text = response.text.strip()
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              exit(1)

          # --- 10. PARSE ---
          parsed = { "TITLE": "", "DESCRIPTION": "", "CATEGORY": display_category, "TAGS": f"{topic},international,news", "IMAGE_KEYWORD": topic, "BODY": "" }
          current_section = None
          
          for line in full_text.splitlines():
              clean_line = line.strip().replace("**", "")
              if clean_line.startswith("TITLE:"): parsed["TITLE"] = clean_line.replace("TITLE:", "").strip()
              elif clean_line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean_line.replace("DESCRIPTION:", "").strip()
              elif clean_line.startswith("CATEGORY:"): parsed["CATEGORY"] = clean_line.replace("CATEGORY:", "").strip()
              elif clean_line.startswith("TAGS:"): parsed["TAGS"] = clean_line.replace("TAGS:", "").strip()
              elif clean_line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean_line.replace("IMAGE_KEYWORD:", "").strip()
              elif clean_line.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          for key in ["TITLE", "DESCRIPTION", "BODY"]:
              text = parsed[key]
              text = text.replace("‚Äî", ", ").replace("‚Äì", ", ").replace("--", ", ")
              text = re.sub(r',\s*,', ',', text)
              parsed[key] = text.strip()

          # --- 11. WORD COUNT VALIDATION ---
          word_count = len(parsed['BODY'].split())
          if word_count < 1300:
              print(f"‚ùå REJECTED: Article too short ({word_count} words). Minimum is 1300.")
              exit(0)

          # --- 12. SAVE ---
          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower())
          slug = re.sub(r'-+', '-', slug).strip('-')
          tag_list = [t.strip() for t in parsed["TAGS"].split(',')]
          
          author_name = "Jonathan Mwaniki"

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_str}"
          author: "{author_name}"
          image: "{image_url}"
          imageCaption: "Image for {parsed['IMAGE_KEYWORD']}"
          imageAlt: "{parsed['IMAGE_KEYWORD']}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps(tag_list)}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Published:</strong> {date_str}</p>
            <p><strong>Author:</strong> {author_name}</p>
            <p><strong>Tags:</strong> {parsed['TAGS']}</p>
          </div>
          """

          import textwrap
          final_file = textwrap.dedent(final_file).strip()
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          
          if len(parsed["TITLE"]) > 5:
              with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
                  f.write(final_file)
              
              memory.append(final_hash)
              with open(memory_path, 'w') as f:
                  json.dump(memory[-200:], f)
                  
              print(f"‚úÖ Published: {slug}.md | {len(parsed['BODY'].split())} words")
          else:
              print("‚ùå Generation failed (empty title)")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish ${{ github.run_id }}"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Content Bot"
          commit_user_email: "actions@github.com"
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}