name: International News (Human Core & GEO)

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'   # 5:00 AM EAT (Politics)
    - cron: '0 5 * * *'   # 8:00 AM EAT (Sports)
    - cron: '0 8 * * *'   # 11:00 AM EAT (Business)
    - cron: '0 11 * * *'  # 2:00 PM EAT (Tech)
    - cron: '0 14 * * *'  # 5:00 PM EAT (Sports)
    - cron: '0 18 * * *'  # 9:00 PM EAT (Entertainment)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client

      - name: Generate article
        env:
          GEMINI_POOL: "${{ secrets.GEMINI_API_KEY }},${{ secrets.GEMINI_API_KEY1 }},${{ secrets.GEMINI_API_KEY2 }},${{ secrets.GEMINI_WRITE_KEY }}"
          APIFY_POOL: "${{ secrets.APIFY_API_TOKEN }},${{ secrets.APIFY_API_TOKEN1 }},${{ secrets.APIFY_API_TOKEN2 }},${{ secrets.APIFY_API_TOKEN3 }},${{ secrets.APIFY_API_TOKEN4 }},${{ secrets.APIFY_API_TOKEN5 }}"
          NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ github.event.inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, hashlib, time
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient

          # --- 1. KEY ROTATION ---
          def get_pool(env_val): return [k.strip() for k in env_val.split(",") if k.strip()]
          gem_pool = get_pool(os.environ.get("GEMINI_POOL", ""))
          api_pool = get_pool(os.environ.get("APIFY_POOL", ""))

          def run_gemini(prompt):
              random.shuffle(gem_pool)
              for k in gem_pool:
                  try:
                      client = genai.Client(api_key=k)
                      return client.models.generate_content(
                          model="gemini-3-flash-preview", 
                          contents=prompt,
                          config=types.GenerateContentConfig(temperature=0.82, max_output_tokens=8192)
                      ).text.strip()
                  except Exception as e:
                      print(f"Key failed: {e}"); continue
              return None

          def run_apify(actor, payload):
              random.shuffle(api_pool)
              for t in api_pool:
                  try:
                      client = ApifyClient(t)
                      r = client.actor(actor).call(run_input=payload)
                      return list(client.dataset(r["defaultDatasetId"]).iterate_items())
                  except Exception as e:
                      print(f"Token failed: {e}"); continue
              return None

          # --- 2. LOGIC ---
          cron_map = {'0 2 * * *': 'politics', '0 5 * * *': 'sports', '0 8 * * *': 'business', '0 11 * * *': 'technology', '0 14 * * *': 'sports', '0 18 * * *': 'entertainment'}
          topic = os.environ.get('MANUAL_TOPIC', '').lower() or cron_map.get(os.environ.get('CRON_SCHEDULE', ''), 'politics')
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          n_api = os.environ.get("NEWSAPI_KEY")
          n_url = f"https://newsapi.org/v2/everything?q={topic}&language=en&sortBy=publishedAt&apiKey={n_api}"
          arts = requests.get(n_url).json().get('articles', [])

          raw_text, f_hash = None, None
          for a in arts[:10]:
              u = a.get('url', '')
              h = hashlib.md5(u.encode()).hexdigest()
              if h not in memory:
                  data = run_apify("apify/website-content-crawler", {"startUrls": [{"url": u}], "maxCrawlPages": 1, "crawlerType": "playwright:firefox"})
                  if data and len(data[0].get('markdown', '')) > 1000:
                      raw_text, f_hash = data[0]['markdown'], h
                      break

          if not raw_text: exit(0)

          # --- 3. THE PERFECTED SEO/GEO/HUMAN PROMPT ---
          # Integrated strategies from provided General Strategies, GEO Fundamentals, and Prompting Tips.
          prompt = f"""You are operating as a human intelligence disguised as conversation. 
          Perform a deep-dive investigative report based on this source:
          SOURCE: {raw_text[:20000]}

          HUMANIZATION STRATEGIES:
          1. Human Core Engine: Blend warmth with finality. Avoid robotic phrasing.
          2. Sentence Rhythm: Mix long, complex sentences with short, blunt fragments to break predictable patterns.
          3. Micro-Signals: Add small human asides like "Honestly," "surprisingly," or "the part that actually matters is...".
          4. No AI Slop: Ban words like 'delve', 'tapestry', 'testament', 'comprehensive', 'vibrant'.
          5. NO Conclusion: Never use 'In conclusion' or 'To summarize'. End with a provocative cliffhanger that leaves the reader questioning the status quo.

          GEO & SEO OPTIMIZATION:
          1. Answer-First Approach: Directly answer the core question of the news in the first two paragraphs.
          2. Quotable Structure: Use clear, punchy headers and short sections to make the content "quotable" for AI search engines.
          3. Contrarian Twist: Identify the mainstream consensus and report from the DIRECTLY OPPOSITE angle.
          4. FAQ Integration: Include a "Key Takeaways" section at the end of the body to match conversational AI queries.

          STRICT RULES: NO EM-DASHES (use commas). UK English. 1600+ words.

          OUTPUT FORMAT:
          TITLE: [Provocative, non-clickbait headline]
          DESCRIPTION: [One blunt, high-GEO sentence]
          CATEGORY: [{topic.title()}]
          TAGS: [tags]
          IMAGE_KEYWORD: [2 words]
          BODY:
          [The full article starting with a direct answer. Use ## headers. End with a cliffhanger.]"""

          output = run_gemini(prompt)
          if not output: exit(1)

          # --- 4. PARSE & CLEAN ---
          parsed = {"TITLE": "", "DESC": "", "IMG": topic, "BODY": ""}
          cur = None
          for line in output.splitlines():
              if line.startswith("TITLE:"): parsed["TITLE"] = line.replace("TITLE:", "").strip()
              elif line.startswith("DESCRIPTION:"): parsed["DESC"] = line.replace("DESCRIPTION:", "").strip()
              elif line.startswith("IMAGE_KEYWORD:"): parsed["IMG"] = line.replace("IMAGE_KEYWORD:", "").strip()
              elif line.startswith("BODY:"): cur = "BODY"
              elif cur == "BODY": parsed["BODY"] += line + "\n"

          # Final Dash/Slop Scrub
          for k in ["TITLE", "DESC", "BODY"]:
              parsed[k] = parsed[k].replace("—", ", ").replace("–", ", ").replace("--", ", ")
              parsed[k] = re.sub(r'(?i)In conclusion,?', '', parsed[k])

          # Image Fallback
          u_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          def get_img(q):
              try:
                  r = requests.get(f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={u_key}", timeout=10)
                  return r.json()['urls']['regular'] if r.status_code == 200 else "https://images.unsplash.com/photo-1504711432869-efd597cdd042"
              except: return "https://images.unsplash.com/photo-1504711432869-efd597cdd042"

          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          final = f"""---
          title: "{parsed['TITLE']}"
          description: "{parsed['DESC']}"
          date: "{datetime.datetime.utcnow().date()}"
          author: "Jonathan Mwaniki"
          image: "{get_img(parsed['IMG'])}"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}
          """

          os.makedirs(os.environ.get("POSTS_DIR"), exist_ok=True)
          with open(os.path.join(os.environ.get("POSTS_DIR"), f"{slug}.md"), "w") as f:
              import textwrap
              f.write(textwrap.dedent(final).strip())

          memory.append(f_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: human-core GEO publish"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'