name: Auto Newser

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports)
    - cron: '0 6 * * *'   # 9AM EAT (Tech)
    - cron: '0 11 * * *'  # 2PM EAT (Finance)
    - cron: '0 14 * * *'  # 5PM EAT (Sports)
    - cron: '0 18 * * *'  # 9PM EAT (Finance)
    - cron: '0 22 * * *'  # 1AM EAT (Tech)
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Required for reading archive for internal links
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Generate article with Gemini
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          APIFY_API_TOKEN5: ${{ secrets.APIFY_API_TOKEN5 }}
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup
          import textwrap

          # --- 1. HELPERS: INTERNAL LINKING & KEY ROTATION ---
          def get_internal_link_context():
              posts_path = os.environ.get('POSTS_DIR')
              if not os.path.exists(posts_path): return ""
              files = [f for f in os.listdir(posts_path) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 3))
              context = "PAST ARTICLES FOR INTERNAL LINKING REFERENCE:\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_path, f), 'r') as content:
                          txt = content.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          title = t_match.group(1) if t_match else f
                          context += f"- Title: {title} | Slug: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          cron = os.environ.get('CRON_SCHEDULE', '')
          key_map = {'0 0 * * *': 'GEMINI_API_KEY', '0 6 * * *': 'GEMINI_API_KEY1', '0 11 * * *': 'GEMINI_API_KEY2', '0 14 * * *': 'GEMINI_API_KEY', '0 18 * * *': 'GEMINI_API_KEY1', '0 22 * * *': 'GEMINI_WRITE_KEY'}
          gemini_key = os.environ.get(key_map.get(cron, 'GEMINI_WRITE_KEY')) or os.environ.get('GEMINI_WRITE_KEY')

          apify_pool = [os.environ.get(v) for v in ['APIFY_API_TOKEN', 'APIFY_API_TOKEN1', 'APIFY_API_TOKEN2', 'APIFY_API_TOKEN3', 'APIFY_API_TOKEN4', 'APIFY_API_TOKEN5'] if os.environ.get(v)]
          selected_apify_token = random.choice(apify_pool) if apify_pool else None

          # --- 2. TOPIC & DISCOVERY ---
          current_hour_utc = datetime.datetime.utcnow().hour
          manual = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          topic = manual if manual in ['sports', 'technology', 'business', 'politics', 'entertainment'] else ('sports' if current_hour_utc in [0, 14] else 'technology' if current_hour_utc in [6, 22] else 'business' if current_hour_utc in [11, 18] else 'politics')
          
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          km_url = f"https://www.kenyamoja.com/{'news' if topic == 'politics' else topic}"
          
          full_raw_content, final_hash = None, None
          try:
              res = requests.get(km_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
              news_items = BeautifulSoup(res.text, 'html.parser').select('li.news-item')
              apify_client = ApifyClient(selected_apify_token)
              
              for item in news_items:
                  link_tag = item.select_one('.news-title a')
                  if link_tag and 'href' in link_tag.attrs:
                      url = link_tag['href']
                      u_hash = hashlib.md5(url.encode()).hexdigest()
                      if u_hash not in memory and url.startswith("http"):
                          run = apify_client.actor("apify/website-content-crawler").call(run_input={"startUrls": [{"url": url}], "maxCrawlPages": 1, "crawlerType": "playwright:firefox", "removeElementsCssSelector": "nav, footer, script, style, .ads, header"})
                          data = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
                          if data and len(data[0].get('markdown', '')) > 800:
                              full_raw_content, final_hash = data[0]['markdown'], u_hash
                              break
          except: exit(0)

          if not full_raw_content: exit(0)

          # --- 3. THE HUMAN CORE & GEO PROMPT ---
          internal_context = get_internal_link_context()
          prompt = f"""OPERATE AS HUMAN INTELLIGENCE DISGUISED AS CONVERSATION.
          Write a 1700-word investigative piece for jonathanmwaniki.co.ke. 
          SOURCE DATA: {full_raw_content[:15000]}
          
          {internal_context}

          STYLE RULES:
          - ANSWER-FIRST (GEO): Directly answer the core news hook in the first two paragraphs.
          - HUMAN RHYTHM: Use varying sentence lengths, blunt fragments, and cynical human asides.
          - INTERNAL LINKING: Mention 1 of the PAST ARTICLES naturally in the text using its slug.
          - NO AI SLOP: Ban 'tapestry', 'delve', 'realm', 'vibrant', 'testament'.
          - NO CONCLUSION: Stop abruptly with a provocative cliffhanger.
          - CONTRARIAN TWIST: Frame this from the DIRECTLY OPPOSITE angle of the mainstream media narrative.

          STRICT: NO EM-DASHES. UK English. ## Headers only.

          OUTPUT FORMAT:
          TITLE: [provocative headline]
          DESCRIPTION: [blunt 155 char summary]
          IMAGE_CAPTION: [cynical 1-sentence caption]
          CATEGORY: [{topic.title()}]
          TAGS: [3 tags]
          IMAGE_KEYWORD: [2 words]
          BODY:
          [Full investigative article. Do not repeat the title.]"""

          # --- 4. GENERATION ---
          client = genai.Client(api_key=gemini_key)
          response = client.models.generate_content(
              model="gemini-3-flash-preview", 
              contents=prompt, 
              config=types.GenerateContentConfig(temperature=0.85, max_output_tokens=8192)
          )
          output_text = response.text.strip()

          # --- 5. ROBUST BLOCK PARSING ---
          def extract(k, t):
              m = re.search(rf"^{k}:\s*(.*)", t, re.M | re.I)
              return m.group(1).strip() if m else None

          parsed = {
              "TITLE": extract("TITLE", output_text) or "The Unseen Angle",
              "DESC": extract("DESCRIPTION", output_text) or "An analytical look at today's news.",
              "CAPTION": extract("IMAGE_CAPTION", output_text) or "Business as usual.",
              "TAGS": extract("TAGS", output_text) or topic,
              "IMG": extract("IMAGE_KEYWORD", output_text) or topic,
              "BODY": ""
          }

          body_m = re.search(rf"BODY:\s*(.*)", output_text, re.S | re.I)
          if body_m: parsed["BODY"] = body_m.group(1).strip()

          # Scrub dashes and AI slop from all keys
          for k in parsed:
              parsed[k] = parsed[k].replace("—", ", ").replace("–", ", ").replace("--", ", ")
              parsed[k] = re.sub(r'(?i)(In conclusion|Summary|To summarize|Conclusion),?.*', '', parsed[k]).strip()

          # --- 6. GEO SCHEMA & IMAGE ---
          date_now = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')
          u_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          def get_img(q):
              try:
                  r = requests.get(f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={u_key}", timeout=10)
                  return r.json()['urls']['regular']
              except: return "https://images.unsplash.com/photo-1504711432869-efd597cdd042"
          
          img_url = get_img(parsed['IMG'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')

          schema = {
              "@context": "https://schema.org",
              "@type": "AnalysisNewsArticle",
              "headline": parsed['TITLE'],
              "description": parsed['DESC'],
              "author": {"@type": "Person", "name": "Belinda Achieng'"},
              "datePublished": date_now,
              "image": [img_url]
          }

          final_md = textwrap.dedent(f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESC'].replace('"', "'")}"
          date: "{date_now}"
          author: "Belinda Achieng'"
          image: "{img_url}"
          imageCaption: "{parsed['CAPTION'].replace('"', "'")}"
          category: "{topic.title()}"
          tags: {json.dumps([t.strip() for t in parsed["TAGS"].split(",")])}
          slug: "{slug}"
          ---

          {parsed['BODY']}

          <script type="application/ld+json">
          {json.dumps(schema, indent=2)}
          </script>
          """).strip()

          out_dir = os.environ.get("POSTS_DIR")
          os.makedirs(out_dir, exist_ok=True)
          with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f: f.write(final_md)
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: human-core publish ${{ github.run_id }}"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: '${{ env.POSTS_DIR }}/*.md .github/scrape_memory.json'
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}