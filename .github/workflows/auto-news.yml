name: Kenyan Tabloid Hunter (SEO & Scandal)

on:
  schedule:
    # Offset minutes to beat GitHub congestion
    - cron: '15 5 * * *'   # 8:15 AM EAT: Morning Political Tea
    - cron: '15 8 * * *'   # 11:15 AM EAT: Relationship Drama
    - cron: '15 11 * * *'  # 2:15 PM EAT: Crime & Safety
    - cron: '15 14 * * *'  # 5:15 PM EAT: Entertainment/Celeb
    - cron: '15 17 * * *'  # 8:15 PM EAT: Tech Scams/Hacks
    - cron: '15 20 * * *'  # 11:15 PM EAT: Late Night Confessions
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Generate Content
        env:
          GEMINI_POOL: "${{ secrets.GEMINI_API_KEY }},${{ secrets.GEMINI_API_KEY1 }},${{ secrets.GEMINI_API_KEY2 }},${{ secrets.GEMINI_WRITE_KEY }}"
          APIFY_POOL: "${{ secrets.APIFY_API_TOKEN }},${{ secrets.APIFY_API_TOKEN1 }},${{ secrets.APIFY_API_TOKEN2 }},${{ secrets.APIFY_API_TOKEN3 }},${{ secrets.APIFY_API_TOKEN4 }},${{ secrets.APIFY_API_TOKEN5 }}"
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup
          import textwrap

          # --- 1. SETUP & ROTATION ---
          def get_clean_pool(v): return [k.strip() for k in v.split(",") if k.strip()]
          gem_pool = get_clean_pool(os.environ.get("GEMINI_POOL", ""))
          apify_pool = get_clean_pool(os.environ.get("APIFY_POOL", ""))

          def get_internal_context():
              posts_dir = os.environ.get("POSTS_DIR")
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 4)) # Sample 4 for better context
              context = "PAST SCANDALS (LINK 1 OF THESE):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- {t}: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def run_gemini(p):
              random.shuffle(gem_pool)
              for k in gem_pool:
                  try:
                      client = genai.Client(api_key=k)
                      return client.models.generate_content(
                          model="gemini-3-flash-preview", 
                          contents=p,
                          config=types.GenerateContentConfig(temperature=0.9, max_output_tokens=8192)
                      ).text.strip()
                  except: continue
              return None

          # --- 2. TOPIC MAPPING (The Pivot) ---
          cron_map = {
              '15 5 * * *': 'politics',         # Corruption/Scandal
              '15 8 * * *': 'relationships',    # Dating Drama
              '15 11 * * *': 'crime',           # Safety/Heists
              '15 14 * * *': 'entertainment',   # Celeb Gossip
              '15 17 * * *': 'technology',      # Scams/Hacks
              '15 20 * * *': 'entertainment'    # Late Night Tea
          }
          topic = os.environ.get('MANUAL_TOPIC', '').lower() or cron_map.get(os.environ.get('CRON_SCHEDULE', ''), 'entertainment')
          
          # KenyaMoja Source Mapping
          km_url_map = {
              'politics': 'https://www.kenyamoja.com/news/kenyans',
              'relationships': 'https://www.kenyamoja.com/lifestyle',
              'crime': 'https://www.kenyamoja.com/news/kenyans', # Often in main news
              'entertainment': 'https://www.kenyamoja.com/entertainment',
              'technology': 'https://www.kenyamoja.com/technology'
          }
          target_url = km_url_map.get(topic, 'https://www.kenyamoja.com/entertainment')

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          
          # --- 3. DISCOVERY ---
          full_raw_text, final_hash = None, None
          try:
              res = requests.get(target_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
              news_items = BeautifulSoup(res.text, 'html.parser').select('li.news-item')
              
              client_apify = ApifyClient(random.choice(apify_pool))

              for item in news_items:
                  link_tag = item.select_one('.news-title a')
                  if link_tag and 'href' in link_tag.attrs:
                      url = link_tag['href']
                      u_hash = hashlib.md5(url.encode()).hexdigest()
                      if u_hash not in memory and url.startswith("http"):
                          print(f"ðŸ•·ï¸ Scrape start: {url}")
                          try:
                              run = client_apify.actor("apify/website-content-crawler").call(run_input={"startUrls": [{"url": url}], "maxCrawlPages": 1, "crawlerType": "playwright:firefox"})
                              data = list(client_apify.dataset(run["defaultDatasetId"]).iterate_items())
                              if data:
                                  text = data[0].get('markdown', data[0].get('text', ''))
                                  if len(text) > 600:
                                      full_raw_text, final_hash = text, u_hash
                                      break
                          except: continue
          except: exit(0)

          if not full_raw_text: exit(0)

          # --- 4. THE PROMPT (Savage & SEO-Safe) ---
          internal_linking = get_internal_context()
          prompt = f"""OPERATE AS A SAVAGE NAIROBI COLUMNIST.
          
          SOURCE MATERIAL: {full_raw_text[:12000]}
          
          {internal_linking}

          MANDATE:
          1. HEADLINE: Must be "Click-Worthy" but honest. Use "Why," "Secret," "Exposed."
          2. ANGLE: Find the drama. If it's politics, find the money. If it's celeb, find the betrayal.
          3. ANSWER-FIRST: Reveal the juice in Paragraph 1.
          4. LINKING: You MUST naturally weave in 1 link from the 'PAST SCANDALS' list.
          5. TONE: Use Nairobi slang sparingly ('Kanairo', 'Tea', 'Wueh') but keep it readable UK English.
          6. NO AI SLOP: Ban 'tapestry', 'delve', 'complex landscape'.

          STRICT: NO EM-DASHES. UK English. ## Headers.

          OUTPUT FORMAT:
          TITLE: [High-Voltage Headline]
          DESCRIPTION: [Tease the scandal]
          IMAGE_CAPTION: [Cynical caption]
          CATEGORY: [{topic.title()}]
          TAGS: [3 tags]
          IMAGE_KEYWORD: [2 metaphorical words]
          BODY: [Full Article]"""

          output = run_gemini(prompt)
          if not output: exit(1)

          # --- 5. PARSE ---
          def extract(key, text):
              m = re.search(rf"^{key}:\s*(.*)", text, re.M | re.I)
              return m.group(1).strip() if m else None

          parsed = {
              "TITLE": extract("TITLE", output) or "Analysis",
              "DESC": extract("DESCRIPTION", output) or "Full story.",
              "CAPTION": extract("IMAGE_CAPTION", output) or "Developing.",
              "IMG": extract("IMAGE_KEYWORD", output) or topic,
              "BODY": ""
          }

          body_match = re.search(r"BODY:\s*(.*)", output, re.S | re.I)
          if body_match:
              parsed["BODY"] = body_match.group(1).strip()
          else:
              parsed["BODY"] = output.split("BODY:")[-1].strip()

          for k in parsed:
              parsed[k] = parsed[k].replace("â€”", ", ").replace("â€“", ", ").replace("--", ", ")
              parsed[k] = re.sub(r'(?i)(In conclusion|To summarize|Conclusion),?.*', '', parsed[k]).strip()

          # --- 6. PUBLISH & SCHEMA ---
          date_now = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          u_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          
          def get_img(q):
              try:
                  r = requests.get(f"https://api.unsplash.com/photos/random?query={q}&orientation=landscape&client_id={u_key}", timeout=10)
                  return r.json()['urls']['regular']
              except: return "https://images.unsplash.com/photo-1504711432869-efd597cdd042"
          
          img_url = get_img(parsed['IMG'])

          # SEO UPGRADE: Person Schema for "Belinda Achieng'"
          schema_json = {
              "@context": "https://schema.org",
              "@type": "AnalysisNewsArticle",
              "headline": parsed['TITLE'],
              "description": parsed['DESC'],
              "datePublished": date_now,
              "author": {
                  "@type": "Person", 
                  "name": "Belinda Achieng'",
                  "jobTitle": "Entertainment Editor"
              },
              "image": [img_url]
          }

          final_md = textwrap.dedent(f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESC'].replace('"', "'")}"
          date: "{date_now}"
          author: "Belinda Achieng'"
          image: "{img_url}"
          imageCaption: "{parsed['CAPTION'].replace('"', "'")}"
          category: "{topic.title()}"
          slug: "{slug}"
          ---

          {parsed['BODY']}

          <script type="application/ld+json">
          {json.dumps(schema_json, indent=2)}
          </script>
          """).strip()

          o_dir = os.environ.get("POSTS_DIR")
          os.makedirs(o_dir, exist_ok=True)
          with open(os.path.join(o_dir, f"{slug}.md"), "w", encoding="utf-8") as f: f.write(final_md)
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"âœ… Published: {slug}")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: kenyan-tabloid publish"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'