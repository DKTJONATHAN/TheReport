name: Auto Newser (Belinda - The Cynic)

on:
  schedule:
    # Runs at minute 15 of every hour from 6 AM to 10 PM EAT
    - cron: '15 3-19 * * *'
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic'
        required: false
        default: ''

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai beautifulsoup4 lxml playwright
          playwright install chromium

      - name: Run Belinda Engine
        env:
          GEMINI_POOL: "${{ secrets.GEMINI_API_KEY }},${{ secrets.GEMINI_API_KEY1 }},${{ secrets.GEMINI_API_KEY2 }},${{ secrets.GEMINI_WRITE_KEY }}"
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright
          import textwrap

          # --- 1. CONFIGURATION ---
          def get_clean_pool(v): return [k.strip() for k in v.split(",") if k.strip()]
          gem_pool = get_clean_pool(os.environ.get("GEMINI_POOL", ""))

          def clean_dashes(text):
              if not text: return ""
              # Replace specific dashes
              text = text.replace('‚Äî', ' - ').replace('‚Äì', '-')
              # Only collapse multiple spaces, NOT multiple newlines
              text = re.sub(r'[ \t]+', ' ', text)
              return text.strip()

          def get_internal_context():
              posts_dir = os.environ.get("POSTS_DIR")
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 4))
              context = "PAST STORIES (LINK 1 OF THESE):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- {t}: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def run_gemini(p):
              random.shuffle(gem_pool)
              models_to_try = ["gemini-3-flash-preview", "gemini-2.0-flash", "gemini-1.5-flash"]
              for model_name in models_to_try:
                  for k in gem_pool:
                      try:
                          client = genai.Client(api_key=k)
                          return client.models.generate_content(
                              model=model_name, 
                              contents=p,
                              config=types.GenerateContentConfig(temperature=0.9, max_output_tokens=8192)
                          ).text.strip()
                      except Exception as e:
                          time.sleep(1)
                          continue
              return None

          # --- 2. TOPIC & DISCOVERY ---
          hour = (datetime.datetime.utcnow().hour + 3) % 24  # Convert to EAT
          topics_pool = ['politics', 'politics', 'crime', 'entertainment', 'gossip', 'lifestyle']
          auto_topic = topics_pool[hour % len(topics_pool)]
          used_topic = os.environ.get('MANUAL_TOPIC', '').lower() or auto_topic

          query_map = {
              'politics': 'Kenya politics OR Ruto OR Raila OR Parliament',
              'crime': 'Kenya crime OR DCI OR police OR scandal',
              'entertainment': 'Kenya entertainment OR Eric Omondi OR Kenyan musician OR celebrity',
              'gossip': 'Kenya gossip OR scandal OR exposed',
              'lifestyle': 'Kenya lifestyle OR Nairobi nightlife OR economy'
          }
          query = query_map.get(used_topic, 'Kenya News')
          rss_url = f"https://news.google.com/rss/search?q={query}+when:6h&hl=en-KE&gl=KE&ceid=KE:en"

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []

          print(f"üîç Topic: {used_topic.upper()}")
          
          try:
              res = requests.get(rss_url, timeout=15)
              soup = BeautifulSoup(res.content, "xml")
              items = soup.find_all("item")
          except: items = []

          # --- 3. FREE SCRAPER ---
          def scrape_article_free(url):
              print(f"üï∑Ô∏è Stealth-Loading: {url}")
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox"])
                  context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                  page = context.new_page()
                  page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

                  try:
                      page.goto(url, timeout=60000)
                      try: page.wait_for_load_state("networkidle", timeout=10000)
                      except: pass
                      
                      soup = BeautifulSoup(page.content(), "html.parser")
                      article_text = ""
                      containers = soup.find_all(['article', 'main', 'div'], class_=re.compile(r'(body|content|entry|post|story)'))
                      for c in containers:
                          paras = c.find_all("p")
                          chunk = "\n\n".join([p.text.strip() for p in paras if len(p.text) > 50])
                          if len(chunk) > 500:
                              article_text = chunk
                              break
                      if not article_text:
                          all_paras = soup.find_all("p")
                          article_text = "\n\n".join([p.text.strip() for p in all_paras if len(p.text.strip()) > 60])
                      return article_text
                  except: return None
                  finally: browser.close()

          # --- 4. FIND & SCRAPE ---
          full_raw_text, final_hash = None, None
          for item in items:
              url = item.link.text
              u_hash = hashlib.md5(url.encode()).hexdigest()
              if u_hash in memory: continue
              
              text = scrape_article_free(url)
              if text and len(text) > 600:
                  full_raw_text = text
                  final_hash = u_hash
                  break
          
          if not full_raw_text: print("‚ùå No new content."); exit(0)

          # --- 5. THE PROMPT (Paragraph Fix) ---
          internal_linking = get_internal_context()
          prompt = f"""OPERATE AS 'BELINDA ACHIENG', A CYNICAL NAIROBI REALIST.
          SOURCE MATERIAL: {full_raw_text[:12000]}
          {internal_linking}
          
          MANDATE: 
          - Point out the bad side. Disillusioned, punchy tone.
          - STRICT FORMATTING: Use Markdown. Break the story into 4-6 paragraphs. Use a double newline between paragraphs.
          
          OUTPUT:
          TITLE: [Cynical Headline]
          DESCRIPTION: [Warning Summary]
          IMAGE_CAPTION: [Caption]
          CATEGORY: [{used_topic.title()}]
          TAGS: [3 tags]
          IMAGE_KEYWORD: [Single Concrete Noun]
          BODY:
          [Article text here with paragraphs]"""

          output = run_gemini(prompt)
          if not output: exit(1)

          # --- 6. PARSE & CLEAN ---
          def extract(key, text):
              m = re.search(rf"^{key}:\s*(.*)", text, re.M | re.I)
              return m.group(1).strip() if m else None

          parsed = {
              "TITLE": extract("TITLE", output) or "Nairobi Updates",
              "DESC": extract("DESCRIPTION", output) or "Full story.",
              "CAPTION": extract("IMAGE_CAPTION", output) or "Developing.",
              "IMG": extract("IMAGE_KEYWORD", output) or used_topic,
              "BODY": output.split("BODY:")[-1].strip()
          }

          # Scrub only metadata, keep line breaks in BODY
          parsed["TITLE"] = clean_dashes(parsed["TITLE"])
          parsed["DESC"] = clean_dashes(parsed["DESC"])
          parsed["BODY"] = parsed["BODY"].replace('‚Äî', ' - ').replace('‚Äì', '-')

          # --- 7. IMAGE & SAVE ---
          date_now = datetime.datetime.utcnow().date().strftime('%Y-%m-%d')
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower()).strip('-')
          u_key = os.environ.get("UNSPLASH_ACCESS_KEY")
          
          def get_img(q):
              fallback = "https://images.unsplash.com/photo-1499364615650-ec387c1470c5?w=1200"
              if not u_key: return fallback
              for term in [q, "Nairobi", "Kenya"]:
                  try:
                      url = f"https://api.unsplash.com/photos/random?query={term}&orientation=landscape&client_id={u_key}"
                      r = requests.get(url, timeout=5)
                      if r.status_code == 200: return r.json()['urls']['regular']
                  except: continue
              return fallback
          
          img_url = get_img(parsed['IMG'])

          final_md = textwrap.dedent(f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESC'].replace('"', "'")}"
          date: "{date_now}"
          author: "Belinda Achieng'"
          image: "{img_url}"
          imageCaption: "{parsed['CAPTION'].replace('"', "'")}"
          category: "{used_topic.title()}"
          slug: "{slug}"
          ---

          {parsed['BODY']}

          <script type="application/ld+json">
          {{
            "@context": "https://schema.org",
            "@type": "AnalysisNewsArticle",
            "headline": "{parsed['TITLE']}",
            "datePublished": "{date_now}",
            "author": {{"@type": "Person", "name": "Belinda Achieng'"}}
          }}
          </script>
          """).strip()

          o_dir = os.environ.get("POSTS_DIR")
          os.makedirs(o_dir, exist_ok=True)
          with open(os.path.join(o_dir, f"{slug}.md"), "w", encoding="utf-8") as f: f.write(final_md)
          
          memory.append(final_hash)
          with open(memory_path, 'w') as f: json.dump(memory[-200:], f)
          print(f"‚úÖ Published: {slug}")
          EOF

      - name: Git Safety Pull
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git stash -u
          git fetch origin main
          git pull origin main --rebase
          git stash pop || echo "Nothing to pop"

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: belinda-cynic publish üá∞üá™"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: 'src/content/posts/*.md .github/scrape_memory.json'