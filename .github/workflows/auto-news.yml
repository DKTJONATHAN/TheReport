name: Auto Newser (Belinda Edition)

on:
  schedule:
    # Optimized Kenyan Scandal Schedule (EAT)
    - cron: '15 5 * * *'   # 8:15 AM
    - cron: '15 8 * * *'   # 11:15 AM
    - cron: '15 11 * * *'  # 2:15 PM
    - cron: '15 14 * * *'  # 5:15 PM
    - cron: '15 17 * * *'  # 8:15 PM
    - cron: '15 20 * * *'  # 11:15 PM
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force topic'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Run Belinda Engine
        env:
          GEMINI_POOL: "${{ secrets.GEMINI_API_KEY }},${{ secrets.GEMINI_API_KEY1 }},${{ secrets.GEMINI_API_KEY2 }},${{ secrets.GEMINI_WRITE_KEY }}"
          APIFY_POOL: "${{ secrets.APIFY_API_TOKEN }},${{ secrets.APIFY_API_TOKEN1 }},${{ secrets.APIFY_API_TOKEN2 }},${{ secrets.APIFY_API_TOKEN3 }},${{ secrets.APIFY_API_TOKEN4 }},${{ secrets.APIFY_API_TOKEN5 }}"
          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup
          import textwrap

          # --- 1. KEY POOLS ---
          def get_clean_pool(v): return [k.strip() for k in v.split(",") if k.strip()]
          gem_pool = get_clean_pool(os.environ.get("GEMINI_POOL", ""))
          apify_pool = get_clean_pool(os.environ.get("APIFY_POOL", ""))

          # --- 2. INTERNAL LINKING ---
          def get_internal_context():
              posts_dir = os.environ.get("POSTS_DIR")
              if not os.path.exists(posts_dir): return ""
              files = [f for f in os.listdir(posts_dir) if f.endswith('.md')]
              if not files: return ""
              sample = random.sample(files, min(len(files), 4))
              context = "PAST SCANDALS (LINK 1 OF THESE):\n"
              for f in sample:
                  try:
                      with open(os.path.join(posts_dir, f), 'r') as c:
                          txt = c.read()
                          t_match = re.search(r'title:\s*"(.*?)"', txt)
                          t = t_match.group(1) if t_match else f
                          context += f"- {t}: /posts/{f.replace('.md', '')}\n"
                  except: continue
              return context

          def run_gemini(p):
              random.shuffle(gem_pool)
              # PRIMARY: gemini-3-flash-preview
              models_to_try = ["gemini-3-flash-preview", "gemini-2.0-flash"]
              
              for model_name in models_to_try:
                  for k in gem_pool:
                      try:
                          client = genai.Client(api_key=k)
                          return client.models.generate_content(
                              model=model_name, 
                              contents=p,
                              config=types.GenerateContentConfig(temperature=0.95, max_output_tokens=8192)
                          ).text.strip()
                      except Exception as e:
                          print(f"Gemini Error ({model_name}): {e}")
                          if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                              print("‚è≥ Quota hit. Sleeping 20s...")
                              time.sleep(20)
                          else:
                              time.sleep(2)
                          continue
              return None

          # --- 3. TOPIC & SOURCE MAPPING (With Fallback) ---
          cron_map = {
              '15 5 * * *': 'politics',
              '15 8 * * *': 'relationships',
              '15 11 * * *': 'crime',
              '15 14 * * *': 'entertainment',
              '15 17 * * *': 'technology',
              '15 20 * * *': 'entertainment'
          }
          initial_topic = os.environ.get('MANUAL_TOPIC', '').lower() or cron_map.get(os.environ.get('CRON_SCHEDULE', ''), 'entertainment')
          
          # Priority: Initial -> Politics (High Vol) -> Entertainment (High Vol)
          topic_queue = [initial_topic, 'politics', 'entertainment']
          topic_queue = list(dict.fromkeys(topic_queue))

          km_url_map = {
              'politics': 'https://www.kenyamoja.com/news/kenyans',
              'relationships': 'https://www.kenyamoja.com/lifestyle',
              'crime': 'https://www.kenyamoja.com/news/kenyans',
              'entertainment': 'https://www.kenyamoja.com/entertainment',
              'technology': 'https://www.kenyamoja.com/technology'
          }

          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          
          # --- 4. DISCOVERY LOOP (Verified Selectors) ---
          full_raw_text, final_hash, used_topic = None, None, None

          for topic in topic_queue:
              target_url = km_url_map.get(topic, 'https://www.kenyamoja.com/entertainment')
              print(f"üîç Attempting Topic: '{topic}' at {target_url}")

              try:
                  res = requests.get(target_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)
                  
                  # SELECTOR LOGIC:
                  # 1. div.views-row -> Standard Drupal container (News, Tech, Ent)
                  # 2. li.news-item -> Legacy container
                  # 3. article -> Generic HTML5
                  news_items = BeautifulSoup(res.text, 'html.parser').select('div.views-row, li.news-item, article')
                  
                  # If no containers found, try finding direct links in content areas
                  if not news_items:
                      news_items = BeautifulSoup(res.text, 'html.parser').select('.view-content a')

                  item_count = 0
                  for item in news_items:
                      # FIND LINK LOGIC:
                      # 1. Look for specific field classes (Drupal)
                      link_tag = item.select_one('.views-field-title a, .field-content a, h3 a, h4 a')
                      
                      # 2. If item is a link itself
                      if not link_tag and item.name == 'a':
                          link_tag = item
                      
                      # 3. Fallback: Any link
                      if not link_tag:
                          link_tag = item.select_one('a')

                      if link_tag and 'href' in link_tag.attrs:
                          url = link_tag['href']
                          u_hash = hashlib.md5(url.encode()).hexdigest()
                          
                          if u_hash in memory or not url.startswith("http"): continue
                          if "facebook.com" in url or "twitter.com" in url: continue
                          
                          print(f"üï∑Ô∏è Found fresh link: {url}")
                          item_count += 1
                          
                          random.shuffle(apify_pool)
                          scrape_success = False
                          
                          for api_key in apify_pool:
                              try:
                                  client_apify = ApifyClient(api_key)
                                  run = client_apify.actor("apify/