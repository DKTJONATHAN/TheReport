name: Auto Newser

on:
  schedule:
    - cron: '0 0 * * *'   # 3AM EAT (Sports) -> KEY
    - cron: '0 6 * * *'   # 9AM EAT (Tech)   -> KEY1
    - cron: '0 11 * * *'  # 2PM EAT (Finance)-> KEY2
    - cron: '0 14 * * *'  # 5PM EAT (Sports) -> KEY
    - cron: '0 18 * * *'  # 9PM EAT (Finance)-> KEY1
    - cron: '0 22 * * *'  # 1AM EAT (Tech)   -> WRITE_KEY
  workflow_dispatch:
    inputs:
      manual_topic:
        description: 'Force a topic (sports, technology, business)'
        required: false
        default: ''

permissions:
  contents: write

env:
  DEFAULT_BRANCH: main
  POSTS_DIR: src/content/posts
  MEMORY_FILE: .github/scrape_memory.json

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ env.DEFAULT_BRANCH }}
          token: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-genai apify-client beautifulsoup4

      - name: Generate article with Gemini
        env:
          # --- GEMINI KEYS ---
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_API_KEY1: ${{ secrets.GEMINI_API_KEY1 }}
          GEMINI_API_KEY2: ${{ secrets.GEMINI_API_KEY2 }}
          GEMINI_WRITE_KEY: ${{ secrets.GEMINI_WRITE_KEY }}
          
          # --- APIFY TOKEN POOL ---
          APIFY_API_TOKEN: ${{ secrets.APIFY_API_TOKEN }}
          APIFY_API_TOKEN1: ${{ secrets.APIFY_API_TOKEN1 }}
          APIFY_API_TOKEN2: ${{ secrets.APIFY_API_TOKEN2 }}
          APIFY_API_TOKEN3: ${{ secrets.APIFY_API_TOKEN3 }}
          APIFY_API_TOKEN4: ${{ secrets.APIFY_API_TOKEN4 }}
          APIFY_API_TOKEN5: ${{ secrets.APIFY_API_TOKEN5 }}

          UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
          MANUAL_TOPIC: ${{ inputs.manual_topic }}
          MEMORY_FILE: ${{ env.MEMORY_FILE }}
          CRON_SCHEDULE: ${{ github.event.schedule }}
        run: |
          python << 'EOF'
          import os, json, datetime, requests, re, random, time, hashlib
          from google import genai
          from google.genai import types
          from apify_client import ApifyClient
          from bs4 import BeautifulSoup

          # --- 1. KEY ROTATION (GEMINI) ---
          cron = os.environ.get('CRON_SCHEDULE', '')
          key_map = {
              '0 0 * * *': 'GEMINI_API_KEY',
              '0 6 * * *': 'GEMINI_API_KEY1', 
              '0 11 * * *': 'GEMINI_API_KEY2',
              '0 14 * * *': 'GEMINI_API_KEY',
              '0 18 * * *': 'GEMINI_API_KEY1',
              '0 22 * * *': 'GEMINI_WRITE_KEY'
          }
          target_key_name = key_map.get(cron, 'GEMINI_WRITE_KEY')
          gemini_key = os.environ.get(target_key_name) or os.environ.get('GEMINI_WRITE_KEY')
          
          print(f"üîë Gemini Key: {target_key_name}")

          # --- 2. KEY ROTATION (APIFY) ---
          # Collect all non-empty tokens into a pool
          apify_pool = []
          for var in ['APIFY_API_TOKEN', 'APIFY_API_TOKEN1', 'APIFY_API_TOKEN2', 'APIFY_API_TOKEN3', 'APIFY_API_TOKEN4', 'APIFY_API_TOKEN5']:
              token = os.environ.get(var)
              if token: apify_pool.append(token)
          
          if not apify_pool:
              print("‚ùå No Apify tokens found!")
              exit(1)
              
          # Select one random token for this run to spread load
          selected_apify_token = random.choice(apify_pool)
          masked_token = selected_apify_token[:5] + "..." + selected_apify_token[-5:]
          print(f"üï∑Ô∏è Apify Token: Selected from pool of {len(apify_pool)} (Using: {masked_token})")

          # --- 3. TOPIC SELECTION ---
          date_str = datetime.datetime.utcnow().date().strftime("%Y-%m-%d")
          current_hour_utc = datetime.datetime.utcnow().hour
          
          manual_input = os.environ.get('MANUAL_TOPIC', '').strip().lower()
          if manual_input in ['sports', 'technology', 'business', 'politics', 'entertainment']:
              topic = manual_input
          else:
              if current_hour_utc == 0 or current_hour_utc == 14: topic = 'sports'
              elif current_hour_utc == 6 or current_hour_utc == 22: topic = 'technology'
              elif current_hour_utc == 11 or current_hour_utc == 18: topic = 'business'
              else: topic = 'politics'
          
          print(f"üéØ Topic: {topic}")

          # --- 4. FETCH CANDIDATES ---
          memory_path = os.environ.get('MEMORY_FILE')
          memory = json.load(open(memory_path)) if os.path.exists(memory_path) else []
          
          km_url = f"https://www.kenyamoja.com/{'news' if topic == 'politics' else topic}"
          
          try:
              res = requests.get(km_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
              soup = BeautifulSoup(res.text, 'html.parser')
              news_items = soup.select('li.news-item')
          except Exception as e:
              print(f"Error fetching source feed: {e}")
              exit(0)

          candidates = []
          for item in news_items:
              link_tag = item.select_one('.news-title a')
              if link_tag and 'href' in link_tag.attrs:
                  url = link_tag['href']
                  # Standardize URL filter
                  if "kenyamoja.com" not in url and url.startswith("http"):
                      url_hash = hashlib.md5(url.encode()).hexdigest()
                      if url_hash not in memory:
                          candidates.append((url, url_hash))

          if not candidates:
              print("No new links found in memory.")
              exit(0)

          # --- 5. SCRAPE LOOP ---
          apify_client = ApifyClient(selected_apify_token)
          full_raw_content = None
          final_url = None
          final_hash = None

          for url, url_hash in candidates:
              print(f"Attempting to scrape: {url}")
              try:
                  run = apify_client.actor("apify/website-content-crawler").call(run_input={
                      "startUrls": [{"url": url}],
                      "maxCrawlPages": 1,
                      "maxCrawlDepth": 0,
                      "crawlerType": "playwright:firefox",
                      "removeElementsCssSelector": "nav, footer, script, style, .ads, header",
                      "proxyConfiguration": {"useApifyProxy": True}
                  })
                  
                  dataset_items = list(apify_client.dataset(run["defaultDatasetId"]).iterate_items())
                  if dataset_items:
                      data = dataset_items[0]
                      text = data.get('markdown', data.get('text', ''))
                      if text and len(text) > 800:
                          full_raw_content = text
                          final_url = url
                          final_hash = url_hash
                          print("Scrape successful!")
                          break
                  print("Scrape empty or too short, trying next...")
              except Exception as e:
                  print(f"Scrape failed: {e}")
                  continue
          
          if not full_raw_content:
              print("All candidates failed.")
              exit(0)

          # --- 6. IMAGE HELPER ---
          def get_real_image(query):
              access_key = os.environ.get("UNSPLASH_ACCESS_KEY")
              if not access_key: return "" 
              
              url = f"https://api.unsplash.com/photos/random?query={query}&orientation=landscape&client_id={access_key}"
              try:
                  resp = requests.get(url, timeout=10)
                  if resp.status_code == 200:
                      return resp.json()['urls']['regular']
              except: pass
              return ""

          # --- 7. PROMPT ---
          hooks = [
             "Straight reporting: What happened, when, where, who, why.",
             "Key facts first: Timeline of events.",
             "Detailed analysis: Numbers and impacts."
          ]
          selected_hook = random.choice(hooks)

          cat_map = { 'sports':'Sports', 'technology':'Technology', 'business':'Business', 'politics':'Politics', 'entertainment':'Entertainment' }
          display_category = cat_map.get(topic, 'General')

          prompt = f'''STRICT REPORTING MODE for jonathanmwaniki.co.ke.

          SOURCE CONTENT:
          {full_raw_content[:15000]}

          TASK: Write FACTUAL news article. NO OPINIONS. NO HYPOTHESIS.
          - Write as the primary and only reporter.
          - NO mentions of "Original story by" or "Source:". 
          - Report in neutral, professional journalist tone.
          - Opening: {selected_hook}

          OUTPUT FORMAT (exact):
          TITLE: [News-style headline, 60 chars]
          DESCRIPTION: [Factual 1-2 sentence summary, 160 chars]
          CATEGORY: [{display_category}]
          TAGS: [3-5 relevant tags]
          IMAGE_KEYWORD: [2-3 words for image]
          BODY:
          [Full article: 1350-1800 words. ## Headers for sections. No em-dashes.]

          RULES:
          - Facts only. If unknown, say "details unclear".
          - No changing topic. Cover THIS story only.
          - UK English, commas not em-dashes.
          - 5-7 sections: Background, Key Developments, Impacts, Reactions, Next Steps.'''

          # --- 8. GEMINI CALL ---
          client = genai.Client(api_key=gemini_key)
          
          print(f"ü§ñ Generating with gemini-3-flash-preview...")
          
          try:
              # Stateless call: No history is retained
              response = client.models.generate_content(
                  model="gemini-3-flash-preview",
                  contents=prompt,
                  config=types.GenerateContentConfig(
                      temperature=0.1,
                      max_output_tokens=8192
                  )
              )
              full_text = response.text.strip()
          except Exception as e:
              print(f"‚ùå Gemini Error: {e}")
              exit(1)

          # --- 9. PARSE ---
          parsed = { "TITLE": "", "DESCRIPTION": "", "CATEGORY": display_category, "TAGS": f"{topic},kenya,news", "IMAGE_KEYWORD": topic, "BODY": "" }
          current_section = None
          
          for line in full_text.splitlines():
              clean_line = line.strip().replace("**", "")
              if clean_line.startswith("TITLE:"): parsed["TITLE"] = clean_line.replace("TITLE:", "").strip()
              elif clean_line.startswith("DESCRIPTION:"): parsed["DESCRIPTION"] = clean_line.replace("DESCRIPTION:", "").strip()
              elif clean_line.startswith("CATEGORY:"): parsed["CATEGORY"] = clean_line.replace("CATEGORY:", "").strip()
              elif clean_line.startswith("TAGS:"): parsed["TAGS"] = clean_line.replace("TAGS:", "").strip()
              elif clean_line.startswith("IMAGE_KEYWORD:"): parsed["IMAGE_KEYWORD"] = clean_line.replace("IMAGE_KEYWORD:", "").strip()
              elif clean_line.startswith("BODY:"): current_section = "BODY"
              elif current_section == "BODY": parsed["BODY"] += line + "\n"

          # Em-Dash scrubber
          for key in ["TITLE", "DESCRIPTION", "BODY"]:
              text = parsed[key]
              text = text.replace("‚Äî", ", ").replace("‚Äì", ", ").replace("--", ", ")
              text = re.sub(r',\s*,', ',', text)
              parsed[key] = text.strip()

          # --- 10. WORD COUNT VALIDATION ---
          word_count = len(parsed['BODY'].split())
          if word_count < 1300:
              print(f"‚ùå REJECTED: Article too short ({word_count} words). Minimum is 1300.")
              exit(0)

          # --- 11. SAVE ---
          image_url = get_real_image(parsed['IMAGE_KEYWORD'])
          slug = re.sub(r'[^a-z0-9-]', '-', parsed["TITLE"].lower())
          slug = re.sub(r'-+', '-', slug).strip('-')
          tag_list = [t.strip() for t in parsed["TAGS"].split(',')]

          final_file = f"""---
          title: "{parsed['TITLE'].replace('"', "'")}"
          description: "{parsed['DESCRIPTION'].replace('"', "'")}"
          date: "{date_str}"
          author: "Belinda Achieng'"
          image: "{image_url}"
          imageCaption: "Image for {parsed['IMAGE_KEYWORD']}"
          imageAlt: "{parsed['IMAGE_KEYWORD']}"
          category: "{parsed['CATEGORY']}"
          tags: {json.dumps(tag_list)}
          featured: true
          draft: false
          slug: "{slug}"
          ---

          # {parsed['TITLE']}

          {parsed['BODY']}

          <div class="article-meta">
            <p><strong>Published:</strong> {date_str}</p>
            <p><strong>Author:</strong> Belinda Achieng'</p>
            <p><strong>Tags:</strong> {parsed['TAGS']}</p>
          </div>
          """

          import textwrap
          final_file = textwrap.dedent(final_file).strip()
          out_dir = os.path.join(os.getcwd(), os.environ.get("POSTS_DIR", "src/content/posts"))
          os.makedirs(out_dir, exist_ok=True)
          
          if len(parsed["TITLE"]) > 5:
              with open(os.path.join(out_dir, f"{slug}.md"), "w", encoding="utf-8") as f:
                  f.write(final_file)
              
              # Update Memory
              memory.append(final_hash)
              with open(memory_path, 'w') as f:
                  json.dump(memory[-200:], f)
                  
              print(f"‚úÖ Published: {slug}.md | {len(parsed['BODY'].split())} words")
          else:
              print("‚ùå Generation failed (empty title)")
          EOF

      - name: Commit and push article
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "content: publish ${{ github.run_id }}"
          branch: ${{ env.DEFAULT_BRANCH }}
          file_pattern: ${{ env.POSTS_DIR }}/*.md
          commit_user_name: "Content Bot"
          commit_user_email: "actions@github.com"
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_GITHUB_TOKEN || secrets.GITHUB_TOKEN }}